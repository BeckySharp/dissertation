\chapter{CONCLUSION\label{chapter:conclusion}}

%\address{briefly remind the reader of the big picture, what this work tells us generally}

In this work, we address the task of question answering (QA), \rev{which in simple terms comes down to querying a knowledge base, which could contain unstructured text or some structured information, to get a desired answer.  With this work we focus on knowledge bases composed of only unstructured text, the more common form of information available.}  Beyond the \rev{specific data} we use here, QA holds promise as a proving ground for developing tools and techniques to be able to navigate the vast sea of text knowledge that is now available, whose sheer size requires automated tools for usage.  

A successful QA approach must go beyond simple lexical lookup to perform inference -- anticipating the desired result based solely on the content of the question and the given background knowledge (i.e., the knowledge base).  As the majority of queries and knowledge base documents are written in natural language, this becomes a bit tougher --  
developing semantic representations of entire phrases or sentences is extremely difficult due to language variety, and yet when considered in isolation (i.e., a bag-of-words), some words are critical for the inference, some are distracting, and some lose much of their meaning without their context.  

Additionally, within a given question set there exists a wide range of question types with different forms of inference needed to solve them (i.e., different information needs).  Consider the difference between a question asking for an \textit{example} of a storm versus a question asking about the \textit{result} of a storm -- each requires a different form of inference to solve.  Consider also a question about the process by which a student might test the effect of sunlight on plants.  Unless there happens to be a single sentence explaining the entire process, many separate facts would need to be connected together to provide a complete explanation.  While this natural language inference is fairly trivial for a human to do, for an automated system it is very much a difficult, unsolved problem.

For QA methods which tackle the problem of natural language inference, we assert that to be truly useful, they must be \textbf{robust}, in that they should be able to handle a large range of questions with varied vocabulary and phrasing and they should be able to operate over large and diverse knowledge bases to allow for ample coverage.  Additionally, we suggest that these methods should be \textbf{interpretable}, or able to explain to a non-expert human user \textit{why} the selected answer was selected.  Without this ability, even when correct, users will hesitate to trust the output of the system.  When incorrect, interpretability is helpful too, as it gives insight into what changes need to be made in order to improve the performance.


Here we proposed a series of approaches that address these two desired attributes, robustness and interpretability, to varying degrees.  \rev{We began with an approach that focuses primarily on robustness --} a monolingual alignment approach in Chapter \ref{chapter:naacl2015} that we extended to be able to train over any free-text resource, rather than only over aligned question-answer pairs, by aligning the words associated with discourse relations found in the text (Section \ref{sec-naacl2015:approach}).  This makes this approach more robust than a traditional QA alignment model, as it extends the corpora over which it can operate, and we demonstrated its success in Section \ref{sec-naacl2015:results}.  \rev{This approach is not particularly interpretable, however, as for selected answers the model provides only a set of learned weights and word associations.  Additionally,} this approach does not, however, address the different information needs present in a question set, rather it attempts to use the same set of learned word association scores for all words in all questions.  Following the intuition that certain discourse relations are better suited to certain information needs (e.g., the \textit{explanation} relation might be more relevant to \textit{Why} questions), this approach could be extended to see whether separate sets of alignments, each individualized to correspond to a single discourse relation, could be leveraged to address different types of questions.   While this is an investigation we leave to future work, we do propose a different approach that uses a model designed to address a specific relation.

In Chapter \ref{chapter:emnlp2016} we describe a framework for QA that would address different question information needs with different models, each tailored to a single need.  We train and evaluate a model for one of these needs, \textit{causality}\rev{, that makes use of word embeddings.  Word embedding models are robust to lexical variation as associations are learned for words not seen together during training, though since they provide only word association scores as output, they are not easily interpretable by users. 
Additionally,} though we demonstrate that this relation-specific model is able to significantly improve upon a general-use model (Section \ref{sec-emnlp2016:results}), in terms of extending this method for use with a variety of questions, we do not particularly address the open-ended issue of determining \textit{what} relations would need to be handled.  Determining the needed relations by examining a given question set would guarantee some degree of relevancy, but it inherently adds bias and may limit the ability to generalize to unseen questions.  In practice, however, this may be the best way to proceed.  Even once a working set of relations is identified, classifying questions for which relation is appropriate would also be necessary and non-trivial, as the expense of manual labels would require using machine learning to train a classifier.
  
Additionally, while it was fairly straightforward to find cause-effect pairs to train the model (since causal language is often fairly explicit, as with phrases such as \textit{X led to Y} or \textit{Y was the result of X}), finding training data for many other relations may be trickier.  This suggests that extracted data for other relations my be noisier than what we extracted here for causality, which was itself a bit noisy.  While we proposed a simple method for mitigating noise, we hypothesize that improving the way that noise is handled would be very beneficial to any system that extends this approach to operate over more relations.  

We also include two approaches that emphasize model interpretability.  For each of these we learn how to select both an answer to a question and a human-readable justification for why the answer is correct.  As we have no labeled training data for justifications, we use the model's performance in the QA task as supervision for both answer and justification selection.  That is, we score all potential justifications for each answer candidate and use the highest scoring as the current justification.  We use the intuition that the best justification for the correct answer should be better than the best justification for the other, incorrect answers (that is, the correct answer should be better supported by the knowledge base than the incorrect answers).  Thus, features from these top-scoring justifications are used to then rank the answer candidates themselves and update the model.  We use this basic framework to rerank answers and learn to select justifications in each of our latter two approaches, detailed in Chapters \ref{chapter:cl2017} and \ref{chapter:emnlp2017}, and in each we demonstrate that the justifications selected are better than those returned by a strong information retrieval baseline, as rated by humans. 
 
In Chapter \ref{chapter:cl2017}, we make use of a directed graph representation of knowledge base sentences based on a simplification of the sentence syntax, then aggregate multiple sentences together.  On the other hand, in Chapter \ref{chapter:emnlp2017} we use only single sentences for our justifications and we use deep learning to learn vector representations of our questions, answers, and justifications.  As a result, with the first approach we are able to use finer-grained features that make use of the structure of the justification graphs themselves, as well as features that characterize the lexical overlap between the aggregated sentences.   In the second approach, our features are coarser -- based only on the surface forms of the sentences (the sequence of words) and the representations learned over them.  However, the shallower representation of the knowledge base sentences means faster processing, which in practice allows for usage over the larger text resources which are arguably necessary for the more complicated questions we handle in Chapter \ref{chapter:emnlp2017} (8th grade versus 3rd-5th grade).        


Each of the models we propose in this work utilizes a bag-of-words approach.
%the discourse-based alignment model and the relation-specific lexical semantic model, 
As mentioned above, often the context of a word is necessary to determine the intended meaning, and many times word order is critical.  For example, consider the difference between a question asking for the state change of \textit{solid to liquid} versus \textit{liquid to solid}.  We are currently unable to handle these questions, though we hope to explore methods that go beyond bag-of-words to robustly handle phrases, sentences, and perhaps even paragraphs to handle arbitrarily more complex questions as well as larger answer and justification texts. 
Much work has been done on determining how to represent longer spans of text, \citep[inter alia,][]{Le2014DistributedRO, Sutskever2014SequenceTS, Pagliardini2017UnsupervisedLO}, but many of the popular techniques (including using Long-Short Term Memory Networks \citep[LSTMs;][]{hochreiter1997long}) require a very large amount of training data and struggle to generalize when presented infrequent or unseen vocabulary and phrasing.  Future work will explore available options for text representation, and perhaps explore new methods.  We hope to also explore deep learning as a means for learning (a) fixed-length representations of the graphlet structures described in Section \ref{sec-cl2017:tag} \citep{jansen2017framing}, and then hopefully (b) alignments between a graphlet representation of questions and answers and the graphlets from potential knowledge base sentence justifications.


While we have proposed several methods to address natural language inference for QA, there is still much to be done for this unsolved problem.  In these methods described here, as well as those we develop in the future, we plan to continue to invest in models which are robust to question variety, so as to maximize the utility of the model, and which are interpretable, and thus able to encourage confidence in the model predictions through providing a human-readable explanation by which a user can evaluate the provided answer.


%\address{summarize everything, don't necessarily repeat what you already said}


%\address{lay out where you’d go next to fill in or extend stuff}

%\address{Say what you learned + what are next steps (for the next student working on this)} 