\section{Experiments}
\label{sec-emnlp2017:experiments}

\subsection{Data and Setup}
We evaluated our model on the set of 8th grade science questions that was provided by the Allen Institute for Artificial Intelligence (AI2) for a recent Kaggle challenge.  The training set contained 2,500 question, each with 4 answer candidates.  For our test set, we used the 800 publicly-released questions that were used as the validation set in the actual evaluation.\footnote{The official testing dataset is not publicly available.}  We tuned our model architectures and hyper-parameters on the training data using five-fold cross-validation (training on 4 folds, validating on 1).  During testing, we froze the model architecture and all hyperparameters and re-trained on all the training data, setting aside a random 15\% of training questions to facilitate early stopping. 
%That is, \todo{explain how early stopping works.}  
% sorry mihai -- no room adn it's common practice in DL.  that sai, if a reviewer wants to know, I'm more than happy to address it with a 9th page...

\subsection{Baselines}
In addition to previous work, we compare our model against two strong IR baselines:
%\paragraph{IR Baseline:} For this baseline, we rank answer candidates by the maximum \emph{tf.idf} document retrieval score using an unboosted query of question and answer terms (see Section \ref{sec-emnlp2017:justretrieval} for retrieval details).
%
%\paragraph{IR$^{++}$:}  This baseline uses the same architecture as the full model, as described in Section \ref{sec-emnlp2017:nn_model}, but with only the IR$^{++}$ feature group.

\begin{itemize}
\item \textbf{IR Baseline:} For this baseline, we rank answer candidates by the maximum \emph{tf.idf} document retrieval score using an unboosted query of question and answer terms (see Section \ref{sec-emnlp2017:justretrieval} for retrieval details).

\item \textbf{IR$^{++}$:}  This baseline uses the same architecture as the full model, as described in Section \ref{sec-emnlp2017:nn_model}, but with only the IR$^{++}$ feature group.
\end{itemize}

\subsection{Corpora}
For our pool of candidate justifications (as well as the scores for our IR baselines) we used the corpora that were cited as being most helpful to the top-performing systems of the Kaggle challenge.  These consisted of short, flash-card style texts gathered from two online resources:  about 700K sentences from StudyStack\footnote{{\scriptsize \url{https://www.studystack.com/}}} and 25K sentences from Quizlet\footnote{{\scriptsize \url{https://quizlet.com/}}}.
From these corpora, we use the top 50 sentences retrieved by the IR model as our set of candidate justifications.  All of our corpora were annotated using using the Stanford CoreNLP toolkit~\cite{manning2014stanford}, the dependency parser of Chen and Manning~\citeyear{chen2014fast}, and the discourse parser of \citet{Surdeanu:15}.
% and indexed with \todo{lucene}. 
%\todo{Explain what a ``document'' is for these resources.} 
%\bs{I do that now in Sect 4.1 -- is that sufficient?}

While our model is able to learn a set of embeddings, we found performance was improved when using pre-trained embeddings, and in this low-data domain, fixing these embeddings to not update during training substantially reduced the amount of model over-fitting.   In order to pre-train domain-relevant embeddings for our vocabulary, we used the documents from the StudyStack and Quizlet corpora, supplemented by the newly released Aristo \textsc{mini} corpus (December 2016 release)\footnote{{\scriptsize \url{http://allenai.org/}}}, which contains 1.2M science-related sentences from various web sources.  The training was done using the \texttt{word2vec} algorithm \cite{mikolov10, mikolov13} as implemented by \citet{levy2014dependency}, such that the context for each word in a sentence is composed of all the other words in the same sentence.  We used embeddings of size 50 as we did not see a performance improvement with higher dimensionality.

\subsection{Model Tuning}
The neural model was implemented in Keras \cite{chollet2015keras} using the Theano \cite{2016arXiv160502688short} backend.  For our feed-forward component, we use a shallow neural network that we lightly tuned to have a single fully-connected layer containing 10 nodes, glorot uniform initialization, a $tanh$ activation, and an L2-regularization of 0.1.  We trained with the RMSProp optimizer \citep{rmsprop},  a learning rate of 0.001, 100 epochs, a batch size of 32, and early stopping with a patience of 5 epochs.  Our loss function used a margin of 1.0.  

We experimented with burn-in, i.e., using the best justification chosen by the IR model for the first mini-batches, but found that models without burn-in performed better, indicating that the model benefited from being able to select its own justification.
 