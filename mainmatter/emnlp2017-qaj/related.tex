\section{Related work}

In many ways, deep learning has become the canonical example of the "black box" of machine learning and many of the approaches to explaining it can be loosely categorized into two types: approaches that try to interpret the parameters themselves (e.g., with visualizations and heat maps \citep{Zeiler2014VisualizingAU,nips15_hermann, Li2016VisualizingAU}, and approaches that generate a human-interpretable metric that is ideally correlated with what is being learned inside the model (e.g., \citet{Lei2016RationalizingNP}). Our approach falls into the latter type -- 
we use our model's reranking of human-readable justifications to give us insight into what the model considers informative for answering questions.  This allows us to see where we do well (Section \ref{sec:justification_results}), and where we can improve (Section  \ref{sec:erroranalysis}).

Deep learning has been successfully applied to many recent QA approaches and related tasks \cite[][inter alia]{Bordes2015LargescaleSQ,nips15_hermann, He2016CharacterLevelQA, dong2015question, Tan2016ImprovedRL}.
%are enticing, and for good reason -- many recent approaches to question answering and related tasks have had much success with various deep learning models \cite[][inter alia]{Bordes2015LargescaleSQ,nips15_hermann, He2016CharacterLevelQA, dong2015question, Tan2016ImprovedRL}.  
However, large quantities of data are needed to train the millions of parameters often contained in these models.  
%Of potentially greater utility in low-data domains, 
Recently, simpler model architectures have been proposed that greatly reduce the number of parameters while maintaining high performance \cite[e.g.,][]{Iyyer2015,chen2016thorough,Parikh2016ADA}.  
%For example, \citet{Iyyer2015}'s show that with their Deep Averaged Network, which replaces complex recurrent neural networks with an average of embeddings and a few, albeit large, dense layers, they improved performance on both a sentiment analysis and a QA task.  For natural language inference, \citet{Parikh2016ADA} used a simpler neural alignment  approach with an attention mechanisms to greatly reduce the size of their model while reaching then state-of-the-art performance.  
We take inspiration from this trend and propose a simple neural architecture for our task to offset the limited available training data. 

Another way to mitigate sparse training data is to include higher-level explicit features.  Like \citet{sachan2016science}, we make use of explicit features alongside features from distributed representations to capture connections between questions, answers, and supporting text.  However, we use a simpler set of features and while they use structured and semi-structured knowledge bases, we use only free-text.  %Additionally, though we also learn to select support from our knowledge base (in some ways similar to \citeauthor{sachan2016science}'s latent answer-entailing structure), since we are explicitly trying to perform \emph{explainable} question answering, here we evaluate the justifications learned by our approach and show that they are significantly better than a  strong IR baseline (Section \ref{sec:justification_results}).   

Our approach to learning justification reranking end-to-end with answer selection is similar to the \citet{jansen2017framing} latent reranking perceptron,  which also operates over free text.  However, our approach does not require decomposing the text into an intermediate representation, allowing our technique to more easily extend to larger textual knowledge bases.  
%ir approach is able to aggregate information from a variety of sources into one justification, our light-weight approach however, does not rely on a large set of heuristics and heavy pre-processing to transform free-text into a structured knowledge base.  

The way we have formulated our justification selection (as a re-ranking of knowledge base sentences) is related to, but distinct from the task of answer sentence selection \cite[][inter alia]{Wang2010ProbabilisticTM, Severyn:12,Severyn:13a,Severyn:13b,Severyn2015LearningTR,wang2015long}.  Answer sentence selection is typically framed as a fully or semi-supervised task for factoid questions, where a correctly selected sentence fully contains the answer text.
%and the problem is designed such that a correctly selected sentence will fully contain the answer text.
Here, we have a variety of questions, many of which are non-factoid.  Additionally, we have no direct supervision for our justification selection (i.e., no labels as to which sentences are good justifications for our answers), motivating our distant supervision approach where the performance on our QA task serves as supervision for selecting good justifications.  Further, we are not actually looking for sentences that \emph{contain} the answer choice, as with answer sentence selection, but rather sentences which close the "lexical chasm" \cite{Berger:00} between question and answer (demonstrated in the example in Table \ref{tab:question_example}). 


%Related work: QA, multiple choice, kaggle challenge, explanation-centered inference, model-specific work (NNs, etc)

%Latest QA papers (latest trends, attn, etc)
%Inspired by this literature (cite DAN \& SNLI, simpler better), but we add this latent layer they don’t have (justification quality). For simple archs: see Danqi Chen at ACL 2016 (similar to DAN but for reading comprehension). 

%Attention models (ACL 2016). Simple especially when you don’t have enough data.

%(visualizations vs analysis of embeddings VS correlated metric EMNLP paper, etc -- interpreting wts if DL is impossible, rather, find correlations betw those and explanatory natural language)

 
%\todo{Add a paragraph comparing against the task of answer sentence selection (see that reviewer from CL, and all those kernel-based papers by Moschitti). The difference is that in our case the answer and its justification arrive from different sources (i.e., the answer is provided in the multiple-choice exam, whereas the justification is extracted from study guides), whereas in previous answer sentence selection work the answer is included in the sentence. In our case we have to deal with bigger ``lexical chasm'' between question/answer/justification.}

%\todo{Discriminative information retrieval for question answering sentence selection(Chen and Van-Durme): Presented a method that selects sentences which contain potential answers for questions from a very large corpus (10\^7 sentences, requiring several thousand questions for training). Their results are dramatically better than Lucene across two datasets and several evaluation measures.}
%(Yih et al.,2013; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013a) and recently using neural networks (Yu et al., 2014; Severyn and Moschitti,2015; Wang and Nyberg, 2015; Yin et al.,2016)

%Answer sentence selection diff because we don't need (or want?) complete answer inclusion.

%Our sentences being selected are unlabeled.

%TAG paper!!! 