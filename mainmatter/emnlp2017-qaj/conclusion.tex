\section{Conclusion}

Here we propose an end-to-end question answering (QA) model that learns to correctly answer questions as well as provide compelling, human-readable justifications for its answers,  despite not having access to labels for justification quality.  We do this by using the question answering task as a form of distant supervision for learning  justification re-ranking.  We show that our accuracy and justification quality are significantly better than a strong IR baseline, while maintaining near state-of-the-art performance for the answer selection task as well.
% and that  the QA task performance is better than several other baselines.  
%This framework can also be extended to allow the model to learn different priorities for justification selection given different question information needs, which we leave to future work.