\section{Introduction}
\label{sec-emnlp2017:intro}

Developing interpretable machine learning (ML) models, that is, models where a human user can \emph{understand} what the model is learning, is considered by many to be crucial for ensuring usability and accelerating progress \cite{craven1996extracting,Kim2015MindTG, letham2015interpretable, Ribeiro2016WhySI}.  
% bs: removed for space, we talk more about this in related work
%As such, it has received much attention in recent years, especially as deep learning and complex architectures have seen dramatic gains in many tasks.
For many applications of question answering (QA), i.e., finding short answers to natural language questions, simply providing an answer is not sufficient. A complete approach must be interpretable, i.e., able to {\em explain} why an answer is correct. 
For example, in the medical domain, a QA approach that answers treatment questions would not be trusted if the treatment recommendation is not explained in terms that can be understood by the human user. 
%, the need for interpretable models:
%QA is one of the most challenging natural language understanding tasks~\cite{Etzioni:11}.  Adding to this challenge is the fact that, 
%for many applications of QA, simply providing an answer is not sufficient. A complete approach must also {\em explain} why an answer is correct. 
%For example, in the medical domain, a QA approach that answers treatment questions would not be trusted if the treatment recommendation is not explained in terms that can be understood by the human user. 
%Despite the importance of explanatory QA, the task of providing justifications as to \emph{why} the extracted answers are correct often takes a backseat to the accuracy of the system, and is not evaluated.
%is often overlooked and not evaluated. 

One approach to interpreting complex models is to make use of human-interpretable information % metric % ms: not a metric...
 generated by the model to gain insight into what the model is learning.  This can be an intermediate representation used by the model, as with the model-generated text spans of \citet{Lei2016RationalizingNP}, that serve as input to another classification network.  
By learning these intermediate representations end-to-end with a downstream task, they are optimized to correlate with what the model learns is discriminatory for the task, and they can be evaluated against what a human would consider to be important.
%\todo{need to define what a downstream task means to use this term}, they are optimized to correlate with what the model learns is discriminatory for the task, and they can be evaluated against what a human would consider to be important.
Here we apply this general framework for model interpretability to QA.
% ms: redundant with paragraph below
% propose a QA model that is able to provide free-text passages as justifications for the answers it selects.  

\begin{table}[t]
\begin{center}
\begin{footnotesize}
\begin{tabularx}{\linewidth}{p{0.13cm}p{6.8cm}}
\multicolumn{2}{p{8cm}}{\textbf{Question:} Which of these is a response to an internal stimulus?} \\
 (A) & A sunflower turns to face the rising sun. \\
 (B) & A cucumber tendril wraps around a wire. \\
 (C) &  A pine tree knocked sideways in a landslide grows upward in a bend. \\
 (\textbf{D}) &\textbf{Guard cells of a tomato plant leaf close when there is little water in the roots .} \\
\\
\multicolumn{2}{p{7.2cm}}{\textbf{Justification:} 
Plants rely on hormones to send signals within the plant in order to respond to internal stimuli such as a lack of water or nutrients. } \\

\end{tabularx}
\end{footnotesize}
\caption{{  Example of an 8th grade science question with a justification for the correct answer.  Note the lack of direct lexical overlap present between the justification and the correct answer, demonstrating the difficulty of the task of finding justifications using traditional distant supervision methods. }}
%space{-6mm} 
\label{tab:question_example}
\end{center}
\end{table}

In this work, we focus on answering multiple-choice science exam questions (Clark \citeyear{clark:2015}; see example in Table~\ref{tab:question_example}). 
This domain is challenging as: (a) approximately 70\% of science exam question shave been shown to require complex forms of inference to solve \cite{clark:2013,jansen-EtAl:2016:COLING}, and (b) there are few structured knowledge bases to support this inference.  
Within this domain, we propose an approach that learns to both select and explain answers, when the only supervision available is for which answer is correct (but not how to explain it).
%Within this domain, we propose an approach that jointly learns to select and explain answers, when the only supervision available is for which answer is correct (but not how to explain it) 
%\todo{PC's comment was remove reference to joint learning so it doesn't get us into trouble.  Do we need to say that we're learning the QA task while providing latent information for the explanation task?}. 
Intuitively, our approach chooses the justifications that provide the most help towards ranking the correct answers higher than incorrect ones.
More formally, our neural network approach alternates between using the current model with max-pooling to choose the highest scoring justifications for correct answers, and optimizing the answer ranking model given these justifications. %\todo{ms: tried to simplify, please check}
%bs - very good! thanks!
% scores all possible justifications for a chosen answer with the current model, and then uses max-pooling 
% re-ranks supporting textual evidence for a chosen answer then uses max-pooling 
%\todo{first reference to deep learning -- needs to be introduced before hand} 
%to choose the highest scoring justification and assign the score of the top-ranked justification to the answer candidate itself for use in the answer selection.  
%such that \todo{expand a bit; mention max-pooling?}.  
Crucially, these reranked texts serve as our human-readable answer justifications, and by examining them, we gain insight into what the model learned was useful for the QA task.   


The specific contributions of this work are:
\begin{enumerate}
\item We propose an end-to-end neural method for learning to answer questions and select a high-quality justification for those answers. 
Our approach re-ranks free-text answer justifications without the need for structured knowledge bases. 
%We formulate this such that the answer ranking supervises the justification re-ranking. 
With supervision only for the correct answers, % but not the quality of their corresponding justifications, % ms: redundant with previous text
we learn this re-ranking through a form of distant supervision -- i.e., the answer ranking supervises the justification re-ranking. 
%bs: I added detail in body of intro
%\todo{this is not very clear; say explicitly what we do with max pooling? or, actually, probably ok to remove since it's redundant to main body of the intro; better explain it there, and just summarize the novelty here}
%Because in our setup we have supervision only for the correct answers, but not for the corresponding answer justifications, .

\item We investigate two distinct categories of features in this ``little data'' domain: explicit features, and learned representations. % \todo{Deep learning jargon, needs to be introduced} \bs{I mainly diagree... and due to serious lack of space I can't intro... so idk.  mihai?}.  % ms: not necessary; well known in this community
We show that, with limited training, explicit features perform far better despite their simplicity. %\todo{Are they simpler? They were generated by a human carefully considering the data}.
%bs: i think we can support their simplicity --they're based on like, proportion of lexical overlap and IR scores.

\item We demonstrate a large (+9\%) improvement in generating high-quality justifications over a strong information retrieval (IR) baseline, while maintaining near state-of-the-art performance on the multiple-choice science-exam QA task, demonstrating the success of the end-to-end strategy.
%Importantly, our approach explains its selected answers significantly better than  a strong IR baseline, demonstrating the success of the joint strategy.
%We demonstrate near state-of-the-art performance on the multiple-choice science-exam QA task: our system would have placed 7th in a recent Kaggle challenge (top 4\%).\footnote{{\scriptsize \url{https://www.kaggle.com/c/the-allen-ai-science-challenge/leaderboard}}} 
%Further, our approach outperforms a recent model that relies on structured knowledge bases~\citep{khot2017tupleinf}, despite having a minimally-tuned, simple architecture, and a smaller set of text-only resources. 
% ms: I took it out because the natural question is why don't we evaluate on that dataset as well?
%which achieves state-of-the-art performance on a related question set. 
\end{enumerate}



%\todo{remove me later -- had weird splitting citation issue again.......... } 

%\todo{move this para to related work}

%The way we have formulated our justification selection (as a re-ranking of knowledge base sentences) is related to, but distinct from the task of answer sentence selection \cite[][inter alia]{Wang2010ProbabilisticTM, Severyn:12,Severyn:13a,Severyn:13b,Severyn2015LearningTR,wang2015long}.  Answer sentence selection is typically framed as a fully or semi-supervised task for factoid questions, and 
%.  Additionally, even when this isn't the case and a semi-supervised approach is necessary, 
%the problem is designed such that a correctly selected sentence will fully contain the answer text.
%the exact span of the answer is fully contained within the correct sentence.   
%Here, we have a variety of questions, many of which are non-factoid.  Additionally, we also have no direct supervision for our justification selection (i.e., we have no labels as to which sentences are good jutsifications for our answers), requiring a form of distant supervision where the performance on our QA task serves as our only signal as to whether or not we are selecting good jutstifcations.   Further, we are not actually looking for sentences that \emph{contain} the answer choice, as with answer sentence selection, but rather sentences which close the "lexical gap" \cite{Berger:00} between question and answer (as demonstrated in the question in Table \ref{tab:question_example}). 


%
%\todo{choose either ``explanation'' or ``justification'' and be consistent}
%\bs{i vote justification and tried to be consistent -- arguments? I'll leave the todo til the end to make sure we're good} 
%
%Question answering (QA), i.e., finding short answers to natural language questions, is one of the most challenging natural language understanding tasks~\cite{Etzioni:11}. Adding to this complexity is the fact that, for many applications of QA, simply providing an answer is not sufficient. A complete approach must also {\em explain} why an answer is correct. 
%For example, in the medical domain, a QA approach that answers treatment questions would not be trusted if the treatment recommendation is not explained in terms that can be understood by the human user. 
%Despite the importance of explanatory QA, the task of providing justifications as to why the extracted answers are correct is often overlooked and not evaluated. 
%
%In this work we argue that answers and their justifications re-enforce each other, and, thus, answer selection and justification should be addressed jointly.
%%bs: removed -- 2 ppl now think this is a bit of a jump/stretch
%%We see this as an initial step towards explainable artificial intelligence.\footnote{For more details see DARPA's Explainable Artificial Intelligence program: {\scriptsize \url{http://www.darpa.mil/program/explainable-artificial-intelligence}}.}
%In particular, we propose a joint approach to select and explain answers to multiple-choice science exam questions~\todo{cite ai2} by re-ranking supporting evidence in the form of free-text. The domain of science exam questions is challenging as: (a) many questions require natural language inference to be answered (see, for example, the question in Table~\ref{tab:question_example}) and (b) there are few structured knowledge bases to support this inference.  
%
%%Additionally, the task of selecting a justification for an answer in this domain is difficult because unlike in typical answer sentence selection tasks with factoid questions (where answer sentences generally completely contain the answer text)~\todo{cite moschitti}, here there is often a ``lexical chasm''~\cite{Berger:00} between answer choices and their justifications.  Also, answer sentence selection is typically a fully-supervised problem but here we have no labels for good justifications.
%The way we have formulated our justification selection (as a re-ranking of knowledge base sentences) is related to, but distinct from the task of answer sentence selection \cite[][inter alia]{Wang2010ProbabilisticTM, Severyn:12,Severyn:13a,Severyn:13b,Severyn2015LearningTR,wang2015long}.  Answer sentence selection is typically framed as a fully supervised task for factoid questions.  Additionally, even when this isn't the case and a semi-supervised approach is necessary, the problem is designed such that the answer is fully contained within the desired KB sentence.   Here, we have a variety of questions, many of which are non-factoid.  We also have no direct supervision for our justification selection, requiring a form of distant supervision (i.e., our joint-learning approach), and to further complicate the task, we are not specifically looking for sentences that \emph{contain} the answer choice, but rather sentences which close the "lexical gap" \cite{Berger:00} between question and answer (as demonstrated in the question in Table \ref{tab:question_example}). 

%Additionally, the task of selecting a justification for an answer in this domain is difficult because unlike in typical answer sentence selection tasks with factoid questions (where answer sentences generally completely contain the answer text)~\todo{cite moschitti}, here there is often a ``lexical chasm''~\cite{Berger:00} between answer choices and their justifications.  Also, answer sentence selection is typically a fully-supervised problem but here we have no labels for good justifications.

%\todo{reformulate -- our justs may or may not contain the answer - may be lexical chasm... not designed to contain the answer, but to address the lex chasm}


%Intro: Explanatory ML. Not â"black boxes". Here we focus on explainable QA. Selecting correct answers is important, but providing justifications as to why those answers are correct is critical in many domains.  This is often overlooked and not evaluated, and limits the ultimate utility and applicability of these systems.

%What we do: focus on science exam questions. hard for two reasons: ``lexical chasm'' which requires natural language inference to be solved. 
%We propose and evaluate an deep learning based approach that jointly extracts and explains answers. 
