\section{Neural justification reranking}
\label{sec-emnlp2017:intro}


Once again, we are concerned with developing interpretable models for question answering.  In Chapter \ref{chapter:cl2017} we proposed a method for question answering that created and ranked human-readable answer justifications using performance on the QA task as the only form of supervision.  We demonstrated that not only did it outperform a strong information retrieval baseline in terms of correctly answering questions, but it also produced a higher percentage of good justifications, as rated by human annotators. The main drawback to the system was simply in the somewhat heavy cost of the sentence processing.  While this processing was able to be done offline (and so only once), the time and size burden still prevents its practical use over extremely large corpora. 

Here, we propose an approach that once again uses answer ranking as distant supervision for learning how to select informative justifications.  As with the previous approach, our justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either.  However, here in this  modified approach we use a  shallower representation of knowledge base sentences and we do not perform aggregation, facilitating usage over much larger resources.  
We additionally extend the linear learning framework used in Chapter \ref{chapter:cl2017} through the use of deep learning (specifically, a non-linear feed-forward neural network), as described in Section \ref{sec-emnlp2017:approach}.  
In making these modifications to increase the robustness of the approach, we purposefully maintain our interpretability -- we continue to rank and return human-readable answer justifications for our model's chosen answers.

In many ways, deep learning has become the canonical example of the "black box" of machine learning and many of the approaches to explaining it can be loosely categorized into two types: approaches that try to interpret the parameters themselves (e.g., with visualizations and heat maps \citep{Zeiler2014VisualizingAU,nips15_hermann, Li2016VisualizingAU}, and approaches that generate human-interpretable information that is ideally correlated with what is being learned inside the model (e.g., \citet{Lei2016RationalizingNP}). Our approach falls into the latter type -- 
we use our model's reranking of human-readable justifications to give us insight into what the model considers informative for answering questions.  This allows us to see where we do well (Section \ref{sec-emnlp2017:justification_results}), and where we can improve (Section  \ref{sec-emnlp2017:erroranalysis}).

Deep learning has been successfully applied to many recent QA approaches and related tasks \citep[][inter alia]{Bordes2015LargescaleSQ,nips15_hermann, He2016CharacterLevelQA, dong2015question, Tan2016ImprovedRL}.
However, large quantities of data are needed to train the millions of parameters often contained in these models.  
Recently, simpler model architectures have been proposed that greatly reduce the number of parameters while maintaining high performance \cite[e.g.,][]{Iyyer2015,chen2016thorough,Parikh2016ADA}.  
%For example, \citet{Iyyer2015}'s show that with their Deep Averaged Network, which replaces complex recurrent neural networks with an average of embeddings and a few, albeit large, dense layers, they improved performance on both a sentiment analysis and a QA task.  For natural language inference, \citet{Parikh2016ADA} used a simpler neural alignment approach with an attention mechanism to greatly reduce the size of their model while reaching then state-of-the-art performance.  
We take inspiration from this trend and propose a simple neural architecture for our task to offset the limited available training data. 


Another way to mitigate sparse training data is to include higher-level explicit features.  Like \citet{sachan2016science}, we make use of explicit features alongside features from distributed representations (see Section \ref{sec-emnlp2017:features}) to capture connections between questions, answers, and supporting text.  However, we use a simpler set of features and while they use structured and semi-structured knowledge bases, we use only free-text.  %Additionally, though we also learn to select support from our knowledge base (in some ways similar to \citeauthor{sachan2016science}'s latent answer-entailing structure), since we are explicitly trying to perform \emph{explainable} question answering, here we evaluate the justifications learned by our approach and show that they are significantly better than a  strong IR baseline (Section \ref{sec:justification_results}).   

Our approach to learning justification reranking end-to-end with answer selection is similar to the \citet[][described in Chapter \ref{chapter:cl2017}]{jansen2017framing} latent reranking perceptron,  which also operates over free text.  However, our approach does not require decomposing the text into an intermediate representation, allowing our technique to more easily extend to larger textual knowledge bases.  

\subsection{Chapter Outline}
The remainder of the chapter is organized as follows.  In Section \ref{sec-emnlp2017:approach} we describe our overall neural approach for QA that reranks answer justifications as an intermediate (and human-interpretable) step in answer selection. In Section \ref{sec-emnlp2017:pipeline} we describe our neural network in more detail.  In that same section, we also describe our set of features designed to combine both learned representations (i.e. embeddings) and explicit features to capture the connection between questions, answers, and answer justifications.  Then, in Section \ref{sec-emnlp2017:experiments} we provide the details of our experimental setup and in Section \ref{sec-emnlp2017:results} we provide our reseults -- with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9\% rated highly relevant) and answer selection (+6\% P@1).  In Section \ref{sec-emnlp2017:erroranalysis} we utilize our model-returned justifications in an error analysis.  Finally, in Section \ref{sec-emnlp2017:conclusions} are our conclusions.


%\todo{Discriminative information retrieval for question answering sentence selection(Chen and Van-Durme): Presented a method that selects sentences which contain potential answers for questions from a very large corpus (10\^7 sentences, requiring several thousand questions for training). Their results are dramatically better than Lucene across two datasets and several evaluation measures.}
%(Yih et al.,2013; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013a) and recently using neural networks (Yu et al., 2014; Severyn and Moschitti,2015; Wang and Nyberg, 2015; Yin et al.,2016)


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\textwidth]{mainmatter/emnlp2017-qaj/arch_overall.png}
\caption{ Architecture of our question answering approach.  
Given a question, candidate answer, and a free-text knowledge base as inputs, we generate a pool of candidate justifications, from which we extract feature vectors.  We use a neural network to score each and then retain \textit{only} the highest-scoring justification from the pool of candidates (i.e., max-pooling), thus selecting the current best justification. This justification score serves as the score for the candidate answer itself.  The red border indicates the components that are trained online. }
\label{fig:arch_overall}
%space{-5mm}
\end{center}
\end{figure}

\section{Approach}
\label{sec-emnlp2017:approach}
One of the primary difficulties with the explainable QA task addressed here is that, while we have supervision for the correct answer, we do not have annotated answer justifications.  
Here we tackle this challenge by using the QA task performance as supervision for the justification reranking, allowing us to 
%extending a neural QA model to jointly learn both how to 
%jointly 
learn to choose both the correct answer and a compelling, human-readable justification for that answer.

Additionally, similar to the strategy \citet{chen2014fast} applied to parsing, we combine representation-based features with explicit features that capture additional information that is difficult to model through embeddings, especially with limited training data.


% ms: avoid "system" too engineering-y
The architecture of our approach is summarized in Figure \ref{fig:arch_overall}.  
Given a question and a candidate answer, we first query an textual knowledge base (KB) to retrieve a pool of potential justifications for that answer candidate.  
For each justification, we extract a set of features designed to model the relations between questions, answers, and answer justifications based on word embeddings, lexical overlap with the question and answer candidate, discourse, and information retrieval (IR) (Section \ref{sec-emnlp2017:features}).
These features are passed into a simple neural network to generate a score for each justification, given the current state of the model.  A final max-pooling layer filters the input, retaining only the top-scoring justification for the candidate answer and this max score is used also as the score for the answer candidate.  In relation to the latent variable perceptron proposed in Section \ref{sec-cl2017:model}, this max-pooling layer is functionally identical to the latent $P$ function that learns to identify the good justifications for a given answer (recall that we implemented $P$ such that it selects exactly one justification at each iteration -- the best-performing, given the current state of the model).
The final candidate answer score (also shown in Figure \ref{fig:arch}) essentially takes the place of $F(q_i, a_j)$ in Equation \ref{eq:F}.     
The system is trained using correct-incorrect answer pairs with a pairwise margin ranking loss objective function to enforce that the correct answer be ranked higher than any of the incorrect answers. 

%The key here is that we use the current state of the model to select the best justification for a given answer candidate from a pool of many candidate justifications.  To do this, we modify the training procedure such that at the start of each epoch \todo{minibatch instead of epoch?}, we first compute a forward pass with each candidate justification to find the top-scoring justification for each candidate answer.
%For a given question, answer candidate, and justification, we combine features based on word embeddings, lexical overlap, discourse, and information retrieval (IR) together in a simple neural architecture to generate a score for the answer candidate.    We then use this selected justification to calculate our gradients for updating the model parameters.  

With this end-to-end approach, the model learns to select justifications that allow it to correctly answer questions.  We hypothesize that, as with the system in Chapter \ref{chapter:cl2017}, this approach enables the model to indirectly learn to choose justifications that provide good explanations as to why the answer is correct. We empirically test this hypothesis in Section \ref{sec-emnlp2017:results}, where we show that
%indeed the model outperforms several strong baselines in both QA performance (correctly answering questions) as well as justification performance (i.e., selecting a justification for an answer that fully explains its connection to the question).
 indeed the model learns to correctly answer questions, as well as to select justifications that fully explain the connection between those answers to the corresponding questions.
% ms: misleading; it reads as if answer selection is better than IR
% better than a strong IR baseline. 
