\section{Conclusion}
\label{sec-emnlp2017:conclusions}

Here we propose an end-to-end question answering (QA) model that learns to correctly answer questions as well as provide compelling, human-readable justifications for its answers,  despite not having access to labels for justification quality.  We do this by using the question answering task as a form of distant supervision for learning  justification re-ranking.  Similar in nature to the model proposed in Chapter \ref{chapter:cl2017}, we differ primarily in the shallower representation of our knowledge base texts (sentences versus graphlets) and the lack of aggregation in forming justifications.  These differences render this model more versatile -- they allow us to utilize larger corpora and consequently handle more challenging question sets that require more diverse background information to answer.   We show that our accuracy and justification quality are significantly better than a strong IR baseline, while maintaining near state-of-the-art performance for the answer selection task as well.  That is, without unduly sacrificing accuracy, with this approach we are able to provide explanations that complete the inference needed to connect questions and their answers.
%for selected answers that complete the inference to connect them to the corresponding questions.

Of the different types of features included in the model, we found that the explicit lexical overlap features were the most useful, and that the discourse and word embedding-based features each provided only a slight performance boost.  For the embedding-based features, this could be due to the limited amount of training data (often, neural networks that successfully learn feature representations operate over orders of magnitude more data than what we have available here).  Indeed, we found that when we attempted to update the word embeddings we ran into issues with  overfitting.  

Regarding the discourse features, with this approach we do not particularly address the different information needs of distinct question types (see \ref{sec:intro_emnlp2016}; cf., Chapter \ref{chapter:emnlp2016}), but intuitively different discourse relations may be more or less relevant to to each of them.  For example, a \textit{cause} relation may be more relevant to a causal question than it is to a definitional question.  Since our approach here does not allow for specialization of the model to different question types, this could be the reason for the underperformance of this discourse-based feature group.
%With this approach we do not particularly address the different information needs of distinct question types (see \ref{sec:intro_emnlp2016}; cf., Chapter \ref{chapter:emnlp2016}).  
Also, as discussed in the error analysis in Section \ref{sec-emnlp2017:erroranalysis}, of the incorrectly answered questions analyzed, 43.3\% required a form of complex inference such as \textit{causal} or \textit{quantitative} reasoning.  We hypothesize that by attempting to apply the same model to \textit{all} question types, we aren't able to specialize enough to handle these more complex information needs.   
While this framework can be extended to allow the model to learn different priorities for justification selection given different question information needs (perhaps similarly to the domain adaptation-based method that is used in Section \ref{sec-cl2017:characterizing}), we leave that extension to future work.  

Additionally, not performing aggregation to construct justifications constitutes a trade-off -- the approach is more lightweight (i.e., faster to run, and able to handle larger corpora within a given resource limit), but it becomes less likely that there exists a \textit{complete} justification for more complicated questions.  Likely there is a way to better balance the two -- perhaps by performing only selected aggregation during pre-processing, for topics that are more likely to need more than a single sentence to express.  Alternatively, perhaps reinforcement learning could be used to learn \textit{when} to add another sentence to an existing candidate justification for a given question and answer.  Again, we leave this exploration to future work.