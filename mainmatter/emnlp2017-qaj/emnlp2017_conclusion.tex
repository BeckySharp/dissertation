\section{Conclusion}
\label{sec-emnlp2017:conclusions}

Here we propose an end-to-end question answering (QA) model that learns to correctly answer questions as well as provide compelling, human-readable justifications for its answers,  despite not having access to labels for justification quality.  We do this by using the question answering task as a form of distant supervision for learning  justification re-ranking.  Similar in nature to the model proposed in Chapter \ref{chapter:cl2017}, we differ primarily in the shallower representation of our knowledge base texts and the lack of aggregation in forming justifications.  These differences allow us to utilize larger corpora and handle more challenging question sets.   We show that our accuracy and justification quality are significantly better than a strong IR baseline, while maintaining near state-of-the-art performance for the answer selection task as well.
% and that  the QA task performance is better than several other baselines.  
With this approach we do not particularly address the different information needs of distinct question types (see \ref{sec:intro_emnlp2016}, cf., Chapter \ref{chapter:emnlp2016}).  However, this framework can also be extended to allow the model to learn different priorities for justification selection given different question information needs, which we leave to future work.