\section{Related Work}
\label{sec:relatedwork}

In one sense, QA systems can be described in terms of their position along a formality continuum ranging from shallow models that rely on information retrieval, lexical semantics, or alignment, to highly structured models based on first order logic (FOL).  

On the shallower end of the spectrum,  QA models can be constructed either from structured text, such as question--answer pairs, or unstructured text.  Alignment models~\cite{Berger:00,echihabi2003noisy,Soricut:06,Riezler:etal:2007,Surdeanu:11,yao2013}  require aligned question--answer pairs for training, a burden which often limits their practical usage (though Sharp et al.~\citeyear{sharp-EtAl:2015:NAACL-HLT} recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure).
Lexical semantic models such as neural-network language models~\cite{jansen14,sultan-etal:2014:TACL,yih13}, on the other hand, have the advantage of being readily constructed from free text.  
Fried et al.~\citeyear{fried2015higher} called these approaches first-order models because associations are explicitly learned, and introduced a higher-order lexical semantics QA model where indirect associations are detected through traversals of the association graph.  
Other recent efforts have applied deep learning architectures to QA to learn non-linear answer scoring functions that model lexical semantics~\cite{Iyyer2014,nips15_hermann}.
However, while lexical semantic approaches to QA have shown robust performance across a variety of tasks, a disadvantage of these methods is that, even when a correct answer is selected, there is no clear human-readable justification for that selection.   

Closer to the other end of the formality continuum, several approaches were proposed to not only select a correct answer, but also provide a formally valid justification for that answer.  For example, some QA systems have sought to answer questions by creating formal proofs driven by logic reasoning~\cite{moldovan2003cogex,moldovan2007cogex,balduccini2008knowledge,maccartney2009natural,liang2013learning,lewis2013combining}, answer-set programming \cite{baral2006using,baral2011towards,baral2012answering,baral2012knowledge}, or connecting semantic graphs~\cite{banarescu2012amr,sharmatowards}. 
However, the formal representations used in these systems, e.g., logic forms, are both expensive to generate 
and tend to be brittle because they rely extensively on imperfect tools such as complete syntactic analysis and word sense disambiguation.
We offer the lightly-structured sentence representation generated by our approach (see Section \ref{sec:tag}) as a shallower and consequently more robust approximation of those logical forms, and show that they are well-suited for the complexity of our questions.
Our approach allows us to robustly aggregate information from a variety of knowledge sources to create human-readable answer justifications.  
It is these justifications which are then ranked in order to choose the correct answer, using a reranking perceptron with a latent layer that models the correctness of those justifications.


Covering the middle ground between shallow and formal representations, learning to rank methods based on tree-kernels~\cite{Moschitti:04} perform well for various QA tasks, including passage reranking, answer sentence selection, or answer extraction~\cite[inter alia]{Moschitti:07,Moschitti:11,Severyn:12,Severyn:13a,Severyn:13b,Tymoshenko:15}. 
The key to tree kernels' success is their ability to automate feature engineering rather than having to rely on hand-crafted features, which allows them to explore a larger representation space. Further, tree kernels operate over structures that encode syntax and/or shallow semantics such as semantic role labeling~\cite{Severyn:12}, knowledge from structured databases~\cite{Tymoshenko:15}, and higher level semantic information such as question category and focus words~\cite{Severyn:13b}.
Here, we similarly use structural features based on syntax, and enriched with additional information about how the answer candidate, the question, and the aggregated justification relate to each other.  
A key difference between our work and methods based on tree kernels is that rather than selecting a contiguous segment of text (sentence or paragraph) our justifications are aggregated from multiple sentences, often from different documents. Because of this setup, we explore content representations that continue to use syntax, but combined with robust strategies for cross-sentence connections. Further, because our justification search space is increased considerably due to the ability to form cross-sentence justifications, we restrict our learning models to linear classifiers that learn efficiently at this scale. However, as discussed, tree kernels offer distinct advantages over linear models. We leave the adaptation of tree kernels to the problem discussed here as future work.



Information aggregation (or fusion) is broadly defined as the assembly of knowledge from different sources, and has been used in several NLP applications, including summarization and QA.  In the context of summarization, information aggregation has been used to assemble summaries from non-contiguous text fragments~\cite[inter alia]{barzilay1999information,barzilay2005sentence}, while in QA, aggregation has been used to assemble answers to both factoid questions~\cite{pradhan2002building} and definitional questions~\cite{blair2003hybrid}.  Critical to the current work, in an in-depth open-domain QA error analysis, Moldovan et al. \citeyear{Moldovan:2003:PIE:763693.763694} identified a subset of questions for which information from a single source is not sufficient, and designated a separate class within their taxonomy of QA systems for those systems which were capable of performing answer fusion. Combining multiple sources, however, creates the need for context disambiguation -- an issue we tackle through the use of question and answer focus words.

Identifying question focus words, a subtask of question decomposition and identifying information needs, was found relevant for QA (especially factoid) early on~\cite[inter alia]{Harabagiu:00,Moldovan:2003:PIE:763693.763694} mainly as a means to identify answer types (e.g., "What is the {\em capital} of France?" indicates the expected answer type is \emph{City}).  
Recently, Park and Croft~\citeyear{Park:2015} have used focus words to reduce semantic drift in query expansion, by conditioning on the focus words when expanding non-focus query words.
Similarly, here, we use focus words (from both question and answer) to reduce the interference of noise in both building and ranking answer justifications.  By identifying which words are most likely to be important for finding the answer, we are able to generate justifications that preferentially connect sentences together on these focus words.  This results in justifications that are better able to remain on-context, and as we demonstrate in Section \ref{sec:experiments}, this boosts overall performance. 

Once the candidate answer justifications are assembled, our method selects the answer which corresponds to the best (i.e., highest-scoring) justification.  We learn which justifications are indicative of a correct answer by extending ranking perceptrons~\cite{Shen:Joshi:2005}, which have been previously used in QA~\cite{Surdeanu:11}, to include a latent layer that models the correctness of the justifications. Latent-variable perceptrons have been proposed for several other NLP tasks~\cite{liang2006end,zettlemoyer2007online,sun2009latent,hoffmann2011knowledge,fernandes2012latent,bjorkelund2014learning}, but to our knowledge, we are the first to adapt them to reranking scenarios. 

Finally, we round out our discussion of question answering systems with a comparison to the famous Watson QA system, which achieved performance on par with the human champions in the Jeopardy! game~\cite{Ferucci:12}.
Several of the ideas proposed in our work are reminiscent of Watson. 
For example, our component that generates text aggregation graphs (Section 5) shares functionality with the Prismatic engine used in Watson. Similar to Watson, we extract evidence from multiple knowledge bases. However, there are three fundamental differences between Watson and this work. 
First, while Watson includes components for evidence gathering and scoring (we call these justifications), it uses a fundamentally different strategy for evidence generation. Similar to most previous work, the textual evidence extracted by Watson always takes the form of a contiguous segment of text~\cite{murdock2012textual},\footnote{Watson also generates ``structured evidence'' which is obtained by converting texts to structured representations similar to logic forms, which are then matched against structured databases for answer extraction. However, this ``logical representation of a clue and then finding the identical representation'' in a database resulted in ``confident answers less than 2\% of the time''~\cite{Ferucci:12}.} whereas our justifications aggregate texts from different documents or knowledge bases. We demonstrate in this work that information aggregation from multiple knowledge bases is fundamental for answering the science exam questions that are our focus (Section 8). 
Second, our answer ranking approach jointly ranks candidate answers and their justifications using a latent-variable learning algorithm, whereas Watson follows a pipeline approach where first evidence is generated, then answers are ranked~\cite{gondek2012framework}. We show in Section 8 that jointly learning answers and their justifications is beneficial. 
Last but not least, Watson was implemented as a combination of distinct models triggered by the different types of Jeopardy! questions, whereas our approach deploys a single model for all questions. Our analysis in Section~\ref{sec:erroranalysis} suggests that there are limits to our simple approach: we measure a ceiling performance for our single-model approach of approximately 70\%. To surpass this ceiling, one would have to  implemented dedicated domain-specific methods for the difficult problems left unsolved by our approach. 

