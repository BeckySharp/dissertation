\section{Conclusion}
\label{sec-cl2017:conclusion}

We have proposed an approach for QA that emphasizes model interpretability, such that producing human-readable justifications for answers, and evaluating answer justification quality, is the critical component\footnote{To increase reproducibility, all the code behind this effort is released as open-source software\footnote{\url{https://github.com/clulab/releases/tree/master/cl2017-qa}}, which allows other researchers to use our entire science QA system as is, or to explore adapting the various components to other tasks. }.
Our interdisciplinary approach to building and evaluating answer justifications includes cognitively-inspired aspects, such as making use of psycholinguistic concreteness norms for focus word extraction, and making use of age-appropriate knowledge bases, which together help move our approach towards approximating the qualities of human inference on the task of question answering for science exams. Intuitively, our structured representations for answer justifications can be interpreted as a robust approximation of more formal representations, such as logic forms~\citep{moldovan2001logic}. However, to maintain robustness, our approach does not evaluate the quality of connections in these structures by their ability to complete a logic proof, but through a reranking model that measures their correlations with good answers.

In our quest for explainability, we have designed a system that generates answer justifications by chaining sentences together. Our experiments showed that this approach improves explain quality, and, at the same time, answers questions out of reach of information retrieval systems, or systems that process contiguous text.  
We evaluated our approach on 1,000 multiple-choice questions from elementary school science exams, and experimentally demonstrated that our method outperforms several strong baselines at both selecting correct answers, and producing compelling human-readable justifications for those answers.  We further validated our three critical contributions: (a) modeling the high-level task of determining justification quality by using a latent variable model is important for identifying both correct answers and good justifications, (b) identifying focus words using psycholinguistic concreteness norms similarly benefits QA for elementary science exams, and (c) modeling the syntactic and lexical structure of answer justifications allows good justifications to be assembled and detected. 

%We performed a detailed error analysis that suggests several important directions for future work. 
%First, though the majority of errors can be addressed within the proposed formalism and by improving focus word extraction, 47.5\% of incorrectly answered questions would also benefit from more complex inference mechanisms, ranging from causal and process reasoning, to modeling quantifiers and negation.
%This suggests that our robust approach for answer justification may complement deep reasoning methods for QA in the scientific domain~\citep{baral2011towards}.
%Second, while our answer justifications are currently short, future justifications might be quite long, and aggregate sentences from knowledge bases of different domains and genres.  In these situations, combining our procedure for constructing justifications with methods that improve text coherence~\citep{barzilay2008modeling} would likely improve the overall user experience for reading and making use of answer justifications from automated QA systems. 
%Finally, our text aggregation graphs currently capture intersentence connections solely through lexical overlap. We hypothesize that extending these structures to capture lexical-semantic overlap driven by word embeddings~\citep{mikolov13}, which have been demonstrated to be beneficial for QA~\citep{yih13,jansen14,fried2015higher}, would also be beneficial here, and increase robustness on small knowledge bases, where exact lexical matching is often not possible.

Our approach seeks to balance robustness with interpretability by utilizing our lightly-structured graphlets for creating answer justifications.  While here we demonstrate the success of this approach in this domain, performing this structured sentence decomposition and aggregation over larger corpora can become quite cumbersome.  For this reason, in Chapter \ref{chapter:emnlp2017} we propose another model which is designed to be even more robust without losing interpretability.  That is, we explore a similar model which continues to rank answer justifications (maintaining interpretability), but which does not perform aggregation and which makes use of only the shallower free-text representations of the knowledge base sentences and accordingly uses a shallower set of explicit feature (rather than those based on focus words and graphlet structure, increasing robustness).  %Additionally, we increase the power of the learning framework by moving from a linear perceptron to a non-linear neural network.

