\section{Conclusion}
\label{sec:conclusion}

We have proposed an approach for QA where producing human-readable justifications for answers, and evaluating answer justification quality, is the critical component.
Our interdisciplinary approach to building and evaluating answer justifications includes cognitively-inspired aspects, such as making use of psycholinguistic concreteness norms for focus word extraction, and making use of age-appropriate knowledge bases, which together help move our approach towards approximating the qualities of human inference on the task of question answering for science exams. Intuitively, our structured representations for answer justifications can be interpreted as a robust approximation of more formal representations, such as logic forms~\cite{moldovan2001logic}. However, our approach does not evaluate the quality of connections in these structures by their ability to complete a logic proof, but through a reranking model that measures their correlations with good answers.

In our quest for explainability, we have designed a system that generates answer justifications by chaining sentences together. Our experiments showed that this approach improves explainability, and, at the same time, answers questions out of reach of information retrieval systems, or systems that process contiguous text.  
We evaluated our approach on 1,000 multiple-choice questions from elementary school science exams, and experimentally demonstrated that our method outperforms several strong baselines at both selecting correct answers, and producing compelling human-readable justifications for those answers.  We further validated our three critical contributions: (a) modeling the high-level task of determining justification quality by using a latent variable model is important for identifying both correct answers and good justifications, (b) identifying focus words using psycholinguistic concreteness norms similarly benefits QA for elementary science exams, and (c) modeling the syntactic and lexical structure of answer justifications allows good justifications to be assembled and detected. 

We performed a detailed error analysis that suggests several important directions for future work. 
First, though the majority of errors can be addressed within the proposed formalism and by improving focus word extraction, 47.5\% of incorrectly answered questions would also benefit from more complex inference mechanisms, ranging from causal and process reasoning, to modeling quantifiers and negation.
This suggests that our robust approach for answer justification may complement deep reasoning methods for QA in the scientific domain~\cite{baral2011towards}.
Second, our text aggregation graphs currently capture intersentence connections solely through lexical overlap. We hypothesize that extending these structures to capture lexical-semantic overlap driven by word embeddings~\cite{mikolov13}, which have been demonstrated to be beneficial for QA~\cite{yih13,jansen14,fried2015higher}, would also be beneficial here, and increase robustness on small knowledge bases, where exact lexical matching is often not possible. 
Finally, while our answer justifications are currently short, future justifications might be quite long, and aggregate sentences from knowledge bases of different domains and genres.  In these situations, combining our procedure for constructing justifications with methods that improve text coherence~\cite{barzilay2008modeling} would likely improve the overall user experience for reading and making use of answer justifications from automated QA systems. 

To increase reproducibility, all the code behind this effort is released as open-source software\footnote{\url{https://github.com/clulab/releases/tree/master/cl2017-qa}}, which allows other researchers to use our entire science QA system as is, or to explore adapting the various components to other tasks. 
