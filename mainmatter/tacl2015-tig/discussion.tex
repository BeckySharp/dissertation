\section{Discussion}
\label{sec:discussion}

To further characterize the performance of our QA approach, we address the following questions: 


%%
%% Comparison with TableILP
%%
%\begin{table}[t]
%\caption{{ \label{font-table} Comparison with other models }} % 
%\small
%\begin{center}
%\begin{tabular}{lccc}
%%\cline{2-3}
%%\begin{tabular}{p{20mm}cc}
%\hline
%\multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Questions} &\multicolumn{1}{c}{P@1} \\
%\cline{2-3}
%\hline
%CR $\cup$ (1G\textsubscript{CT} + 2G\textsubscript{CT}) $\cup$ 3G\textsubscript{CT} 			 	& 1000		&	44.6\%	\\
%Khashabi et. al (2016)																				& 200 		&	45.6\%	\\
%
%\end{tabular}
%\label{tab:tableilp}
%\end{center}
%\end{table}

{\flushleft {\bf How does performance compare with methods using manually constructed knowledge bases?}} The TAG system automatically aggregates sentences from six free-text corpora first by building graphlets from those sentences using syntactic dependencies, then connecting those graphlets together into multisentence text aggregation graphs that are then used to both answer questions and provide a compelling human-readable justification for the selected answers.  Recently, Khashabi et al. \citeyear{Khashabi2016QuestionAV} demonstrated that graphs for elementary science QA can also be constructed using a semistructured knowledge base of tables.  In this formalism, dozens of themed tables are manually or semi-automatically constructed, each around a particular theme.  A table's theme is encoded in it's columns, i.e., a table for the color of objects contains two rows, one for the object of interest (e.g., ``leaf''), and another for it's color (e.g., ``green''), while separate instances (e.g., leaf -- green, trunk -- brown) are encoded as different rows.  Each table has between two and five columns.  

The TableILP algorithm answers questions by chaining facts between different table rows, starting from a row that contains question terms, then traversing to a new table row that contains some lexical overlap with the previous row(s), until answer terms are found.  The TAG and TableILP systems are conceptually similar, with the central differences being: (1) The TableILP table row is roughly equivalent to a TAG graphlet with flat structure, and limited to 2--5 information nuggets containing only single terms, (2) TAG graphlets are read automatically from free text corpora, where TableILP tables are largely manually constructed, with methods to automate this construction being actively developed, and (3) the traversal algorithms are different, with TableILP graph building being modelled as an integer linear programming (ILP) problem which finds paths that maximize QA performance.  

The TableILP system reported by Khashabi et al. \citeyear{Khashabi2016QuestionAV} contains 69 tables containing a total of 7,600 rows, with 64 of these tables (approximately 5,000 rows) designed around material in the study guides and a development corpus, and the remaining 2,600 rows distributed amoung 4 automatically constructed tables.  On a corpus of 200 questions drawn from the 1,000 questions used here, TableILP achieved a score of 45.6\% P@1\footnote{We wish to thank Khashabi et al. \citeyear{Khashabi2016QuestionAV} for providing us with this performance figure.}, compared to the 44.6\% P@1 from the best-performing TAG model in Table~\ref{tab:combinedmodels}. The performance difference between these systems is likely not statistically significant.\footnote{We did not have access to system output. However, in our experiments on this dataset, we observed that only differences in P@1 scores of 3\% or higher (absolute) tend to be statistically significant at $p < 0.05$.}

We view these systems as complementary, converging, and with each capable of exploring different aspects of graph-based inference for science QA.  While the TAG focuses on automatically building graphs from free text, this is currently a challenging and noisy process, and as we have shown in Table~\ref{tab:pathlength} and Fried et al.~\citeyear{fried2015higher}, highly susceptable to inference drift as the amount of information required to be aggregated becomes large.  On the other hand, building graphs from manually constructed knowledge bases allow us to investigate the graph-building process in isolation, reducing inference drift due to noise, and further moving this area forward.  

%
% Performance by grade level
%
\begin{table}[t]
\caption{{ \label{font-table} Precision@1 by grade level. }} % 
\small
\begin{center}
\begin{tabular}{lccc}
%\cline{2-3}
%\begin{tabular}{p{20mm}cc}
\hline
\multicolumn{1}{c}{Grade Level} & \multicolumn{1}{c}{Questions} &\multicolumn{1}{c}{CR} & \multicolumn{1}{c}{TAG}  \\
\cline{2-3}
\hline
Grade 3 	& 60		&	28.3\%		& 49.2\%	\\
Grade 4		& 69 	&	50.7\%		& 41.3\%	\\
Grade 5		& 871	&	40.2\%		& 42.6\%	\\

\end{tabular}
\label{tab:gradelevel}
\end{center}
\end{table}


{\flushleft {\bf How does performance vary by grade level?}} The question corpus contains third, fourth, and fifth grade questions.  A human with a level of knowledge equivalent to a fourth grade science student might be expected to show better performance for the simpler third grade questions, and decreasing performance as question difficulty increases from fourth to fifth grades.  Table~\ref{tab:gradelevel} shows P@1 performance by grade level for both the CR and best performing TAG model (1G\textsubscript{CT} + 2G\textsubscript{CT}).  The TAG model shows decreasing performance as question difficulty increases, dropping from 49\% for third grade questions to 42\% for fourth and fifth grade questions.  The CR baseline, however, displays a qualitatively different pattern, with a peak performance of 51\% for fourth grade questions, and {\em near chance} performance for third grade questions. 
We believe that observing such a pattern in performance may suggest that the TAG model is a closer approximation of human inference than the baseline based solely on information retrieval.  Here, the relatively small number of third and fourth grade questions prevents us from drawing any conclusions, but suggests that crafting question sets to allow evaluating the distribution of performance by grade level may provide a further measure of comparison between human and machine performance. 



%
% Justification performance by knowledge resource
%
\begin{table}[t]
\caption{{ \label{font-table} Most useful knowledge resources for justifications classified as "good".}}
\small
\begin{center}
\begin{tabular}{lccc}
%\cline{2-3}
%\begin{tabular}{p{20mm}cc}
\hline
\multicolumn{1}{l}{Resource} & \multicolumn{1}{c}{Sentences} &\multicolumn{1}{c}{CR} & \multicolumn{1}{c}{TAG}  \\
\cline{2-3}
\hline
Barrons SG 			& 1,200		&	39.3\%		& 43.0\%	\\
Flashcards			& 283		&	16.2\%		& 8.2\%	\\
Teacher's Guide		& 302		&	7.1\%		& 7.0\%	\\
Virginia SG			& 1,314		&	9.1\%		& 9.2\%	\\
Science Dictionary	& 733		&	20.8\%		& 17.8\%	\\
Simple Wiktionary	& 17,473		&	7.5\%		& 14.8\%	\\

\end{tabular}

\label{tab:justificationknowledgeresources}
\end{center}
\end{table}

{\flushleft {\bf Which knowledge resources are generating the most useful answer justifications?}} Shown in Table~\ref{tab:justificationknowledgeresources}, the Barron's Study Guide (SG) contributes more of the \emph{good} justification sentences than any other source, followed by the science dictionary, then the other resources.  Interestingly, the Simple Wicktionary contributes the fewest sentences to the \emph{good} justifications for the CR system (7.5\%), but for the TAG system it is the third largest contributor (14.8\%).  That is, while the CR system is typically unable to find a \emph{good} justification from the Wiktionary, likely owing to it's general nature, the TAG system is able to successfully aggregate these sentences with sentences from other domain-specific sources to build complete justifications.

The vast majority of the \emph{good} justifications generated by the TAG system are aggregates from non-adjacent text: 67\% of the justifications aggregate sentences from \emph{different} corpora, 30\% aggregate non-adjacent sentences within a \emph{single} corpus, while only 3\% of \emph{good} justifications contain sentences that were adjacent in their original corpus. 
This is clear evidence that information aggregation (or fusion) is fundamental for answer justification.


{\flushleft {\bf How orthogonal is the performance of the TAG model when compared to CR?}} Both the TAG and CR models use the same knowledge resources, which on the surface suggests the models may be similar, answering many of the same questions correctly.  The voting models in Table~\ref{tab:combinedmodels} appear to support this, where combining the TAG and CR models increases performance by just under 2\% P@1 over the best-performing TAG model.  To investigate this, we conducted an orthogonality analysis to determine the number of questions both models answer correctly, and the number of questions each model uniquely answers correctly.

Comparing the TAG (1G\textsubscript{CT} + 2G\textsubscript{CT}) and CR models, nearly half of questions are answered correctly by one model and incorrectly by the other.  When combined into a two-way voting model, this causes a large number of ties -- which, resolved at chance, would perform at 42\%, with ceiling performance (i.e., all ties resolved correctly) at 60\%.  This indicates that while the TAG and CR models share about half of their performance, each model is sensitive to different kinds of questions, suggesting that further combination strategies between TAG and CR are worth exploring.

