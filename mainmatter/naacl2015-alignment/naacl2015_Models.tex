

%----------------------TABLE FORMAT FROM TACL 2015--------
\begin{table*}[t]{}
        \centering
        \begin{tabular}{p{1cm}p{10cm}}
		\hline        
        \multicolumn{2}{l}{\bf Alignment Models} \\
        \hline
        \multicolumn{2}{l}{Global Alignment Probability:} \\
		  &  p(Q$|$A) according to IBM Model 1 \citep{Brown:93} \\ 
		\\        
        \multicolumn{2}{l}{Jenson-Shannon Distance (JSD) Features:} \\         
         {  } & {Pairwise JSDs were found between the probability distribution of each content word in the question and those in the answer.  The \textbf{mean, minimum, and maximum JSD values} were used as features. Additionally, composite vectors were formed which represented the entire question and the entire answer and the \textbf{overall JSD} between these two vectors was also included as a feature. See \citet{fried2015higher} for additional details.} \\
		\\        
        \hline
		 \multicolumn{2}{l}{\bf Embedding Model} \\
		 \hline
		 \multicolumn{2}{l}{Cosine Similarity Features:} \\        
         {  } & {Similar to \citet{jansen14}, we include as features the {\bf maximum and average pairwise cosine similarity} between question and answer words, as well as the {\bf overall similarity} between the composite question and answer vectors.} \\
        \end{tabular}
        \caption{Feature descriptions for alignment models and the embedding baseline.}
        \label{tab:Features}
	
\end{table*}

%\begin{table*}[h]{}
%
%%\begin{scriptsize}
%
%        \centering
%        %\resizebox{7.9cm}{!}{
%        {
%        %\begin{tabular}{p{1cm}|p{3cm}|l}
%        \begin{tabular}{p{0.3cm}|p{5cm}|p{8cm}}
%        \hspace*{-7pt}  & Feature Group & Feature Descriptions \\
%        \toprule
%        \multirow{10}{*}
%        {\rotatebox[origin=c]{90}{~~~~~~~~~Alignment Models}}
%        & { Global Alignment Probability } & p(Q$|$A) according to IBM Model 1 \cite{Brown:93}\\ 
%        & {} & {}\\
%        & { Jenson-Shannon Distance (JSD) } & {Pairwise JSDs were found between the probability distribution of each content word in the question and those in the answer.  The \textbf{mean, minimum, and maximum JSD values} were used as features. Additionally, composite vectors were formed which represented the entire question and the entire answer and the \textbf{overall JSD} between these two vectors was also included as a feature. See Fried et. al \citeyear{fried2015higher} for additional details.} \\
%        \midrule
%        \multirow{10}{*}
%        {\rotatebox[origin=c]{90}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~RNNLM}}
%        & { Cosine Similarity } & {Similar to Jansen et al.~\citeyear{jansen14}, we include as features the {\bf maximum and average pairwise cosine similarity} between question and answer words, as well as the {\bf overall similarity} between the composite question and answer vectors.} \\
%        \bottomrule
%        \end{tabular}
%        }
%        %}
%
%%\end{scriptsize}
%
%        %\caption{$10$ most important features of each sieve.}
%        \caption{Feature descriptions for alignment models and RNNLM baseline.}
%        \label{tab:Features}
%	\vspace{-6mm}
%
%\end{table*}

%------------------END COPIED TABLE----------------
%space{-1mm}
\section{Models and Features}
\label{sec-naacl2015:models}

We evaluate the contribution of these alignment models using a standard reranking architecture~\citep{jansen14}.
The initial ranking of candidate answers is done using a shallow information retrieval (IR) component.\footnote{We use the same cosine similarity between question and answer lemmas as \citet{jansen14}, weighted using {\em tf.idf}.  Please see \citet{jansen14} for detailed architecture.}  
Then, these answers are reranked using a more expressive model that incorporates alignment features alongside the IR score.  As a learning framework we use \svmr , a Support Vector Machine tailored for ranking.\footnote{ \url{http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html}}
We compare this alignment-based reranking model against one that uses a state-of-the-art recurrent neural network language model \citep{mikolov10,mikolov13}, which has been successfully applied to QA previously~\citep[e.g.,][]{yih13}.


{\flushleft {\bf Alignment Model:}}  The alignment matrices were generated with IBM Model 1 \citep{Brown:93} using GIZA++~\citep{och03}, and the corresponding models were implemented as per \citet{Surdeanu:11} with a global alignment probability, i.e., the conditional probability of observing a question given an answer.
% ms: the global prob. operates for the whole Q and A!
%question word given an answer word.
\footnote{Within \citet{Surdeanu:11} framework, we lightly tuned the smoothing parameter, $\lambda$, to 0.4 and we redistributed the probability mass for each word such that the probability of a word translating to itself was 0.5.}  
We extend this alignment model with features from \citet{fried2015higher} that treat each (source) word's probability distribution (over destination words) in the alignment matrix as a distributed semantic representation, and make use of the Jensen-Shannon distance (JSD)\footnote{Jensen-Shannon distance is based on Kullback-Liebler divergence but is a distance metric (finite and symmetric).} between these conditional distributions.  A summary of all these features is shown in Table \ref{tab:Features}.% to derive features representing the {\bf minimum, maximum, and average JSDs} between question and answer words, as well as the {\bf overall JSD} between the composite question and answer distributions. 
%Using the Jensen-Shannon distance (JSD) between conditional distributions, content words in each question were compared with the content words in its answers.  Features were made from the maximum, minimum, and average JSDs.  Composite distributions were also created for the entire question and for each entire candidate answer, and the JSD between these composite distributions was used as a feature. 


{\flushleft {\bf Embed:}} We learned word embeddings using the \texttt{word2vec} recurrent neural network language model of \citet{mikolov13}, and include the cosine similarity-based features described in Table \ref{tab:Features}. %Similar to Jansen et al.~\citeyear{jansen14}, we include as features the {\bf maximum and average pairwise cosine similarity} between question and answer words, as well as the {\bf overall similarity} between the composite question and answer vectors. 

%{\flushleft {\bf Hyperparameters}}We used the following very lightly tuned hyperparameters: for \svmr, we used c = 0.1 and for IBM model 1, lambda = 0.4 and self-association of 0.5. \todo{re-write this!! or spread to appropriate places} 
