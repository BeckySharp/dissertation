\chapter{INTRODUCTION\label{chapter:introduction}}

--Start by explaining why NLI and QA are fundamental NLP tasks. Give examples to search, and personal assistants.

--Natural language inference (NLI)

--while FOL methods are attractive for their formality, it is this same formality that renders them too brittle to be of much use outside of small, toy domains.

-- Here we focus on approximating this inference using natural language instead of formal representations (e.g. ccg parses, etc)

-- methods that can be applied in much broader domains, with an emphasis on semi- and distant supervision, rather than needing a large amount of hand-generated training data.

--With the relaxation of the formality, the danger is in losing the explainability, and so we use/include models that use a human-readable intermediate output generated by the model to provide an explanation for the inference performed by the model.

%-- We try/test our NLI methods in the domain of Question answering (QA):
%QA hard bc of 

% emnlp2016 intro
Question answering (QA), i.e., finding short answers to natural language questions, is one of the most important but challenging 
tasks on the road towards natural language understanding~\cite{Etzioni:11}. 
%-- Require bridging lexical chasm
Unlike search or information retrieval, answers infrequently contain lexical overlap with the question (e.g. {\em What should we eat for breakfast? -- Zoe's Diner has good pancakes}), and require QA models to draw upon more complex methods to bridge this "lexical chasm" \cite{Berger:00}.  These methods range from robust shallow models based on lexical semantics, to deeper, explainably-correct, but much more brittle inference methods based on first order logic.  

%-- One approach to bridging lexical chasm is use of monolingual alignment models, but: 
%-- these are expensive to train (need lots of gold QA pairs)
% naacl2015 intro
Berger et al.~\citeyear{Berger:00} proposed that this "lexical chasm" might be partially bridged by repurposing statistical machine translation (SMT) models for QA. Instead of translating text from one language to another, these monolingual alignment models learn to translate from question to answer\footnote{In practice, alignment for QA is often done from answer to question, as answers tend to be longer and provide more opportunity for association~\cite{Surdeanu:11}.}, learning common associations from question terms such as {\em eat} or {\em breakfast} to answer terms like {\em kitchen, pancakes, or cereal}.

While monolingual alignment models have enjoyed a good deal of recent success in QA (see related work), they have expensive training data requirements,  
requiring a large set of aligned in-domain question-answer pairs for training.
%In most domains these pairs are expensive to generate, and one of the current methodological challenges in QA is locating or building high-quality QA pairs for training and testing. Even large open-domain international evaluations and workshops such as the Text REtrieval Conference (TREC)\footnote{\url{http://trec.nist.gov}} and the Cross Language Evaluation Forum (CLEF),\footnote{\url{http://www.clef-initiative.eu}} are often limited to sets of a few hundred factoid questions, many of which are highly related.  As a result, for open domain QA one often makes use of Community Question Answering (CQA) data from websites such as Yahoo! Answers or Stack Overflow, which offer tens of thousands of questions, but of highly variable quality.  
For low-resource languages or specialized domains like science or biology, often the only option is to enlist a domain expert to generate gold QA pairs --  a process that is both expensive and time consuming.  All of this means that only in rare cases are we accorded the luxury of having enough high-quality QA pairs to properly train an alignment model, and so these models are often underutilized or left struggling for resources. 

%-- We propose a method for generating artificial training data (can be thought of as a form of distant supervision?? If you squint?) (NAACL2015)
% naacl2015 intro
Making use of recent advancements in discourse parsing \cite{feng12}, in Chapter \ref{chapter:naacl2015} we address this issue, and investigate whether alignment models for QA can be trained from artificial question-answer pairs generated from discourse structures imposed on free text. \todo{work in that can be thought of as a form of distant supervision?? If you squint?}
% by imposing structure on inexpensive free text resources instead of using QA pairs.  
We evaluate our methods on two corpora, generating alignment models for an open-domain community QA task using Gigaword\footnote{LDC catalog number LDC2012T21}, and for a biology-domain QA task using a biology textbook. 


-- Diff info needs and so the “manner” by which the lexical chasm is bridged should hopefully be robust to that

This alignment approach for QA can be considered as falling into a larger group of approaches which prefer answers that are closely related to the question, where the relatedness is determined by the associations of the alignment model or by associations provided by other lexical semantic models such as word embeddings~\cite{yih13,jansen14,fried2015higher}. 
While appealing for its robustness to natural language variation, this one-size-fits-all category of approaches does not take into account the wide range of distinct question types that can appear in any given question set, and that are best addressed individually~\cite{chu2004ibm,ferrucci2010building,clark2013study}.  

%These don’t address diff types of inference/info needs:
%-- We propose a framework for learning customized alignments/associations for a specific info need using semi-supervised methods (EMNLP2016)
Given the variety of question types, we suggest that a better approach is to look for answers that are related to the question \emph{through the appropriate relation}, e.g., a causal question should have a cause-effect relation with its answer.
Adopting this view, and working with embeddings as a mechanism for assessing relationship, we address a key question: how do we train and use task-specific embeddings cost-effectively? 
In Chapter \ref{chapter:emnlp2016}, using causality as a use case, we answer this question with a framework for producing causal word embeddings with minimal supervision, and a demonstration that such task-specific embeddings significantly benefit causal QA. 

% ----------CONTRIBUTIONS-
The contributions of this work are:
\begin{enumerate}

% naacl 2015
\item We demonstrate that by exploiting the discourse structure of free text, monolingual alignment models can be trained to surpass the performance of models built from expensive in-domain question-answer pairs. 

\item We compare two methods of discourse parsing: a simple sequential model, and a deep model based on Rhetorical Structure Theory (RST)~\cite{mann88}.  We show that the RST-based method captures within and across-sentence alignments and performs better than the sequential model, but the sequential model is an acceptable approximation when a discourse parser is not available.  

\item We evaluate the proposed methods on two corpora, including a low-resource domain where training data is expensive (biology).

\item We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-the-art neural network language models in low resource domains.
\end{enumerate}

% emnlp 2016
{\flushleft {\bf (1)}} 
A methodology for generating causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns, e.g., {\em X causes Y}. 
%We propose a method to generate knowledge resources for causal questions 
%We demonstrate that knowledge resources for causal questions can be generated by bootstrapping cause-effect pairs extracted from free text using a small set of high-precision patterns, e.g., {\em X causes Y}. 
We then train dedicated embedding (as well as two other distributional similarity) models over this data. \citet{levy2014dependency} have modified the algorithm of\citet{mikolov2013distributed} to use an arbitrary, rather than linear, context. Here we make this context task-specific, i.e., the context of a cause is its effect.
%embedding models (as well as alignment and convolutional neural network models) over this data. 
Further, to mitigate sparsity and noise, our models are bidirectional, and noise aware (by incorporating the likelihood of noise in the training process). 
%We achieve the latter by weighting the examples based on the likelihood that they are truly causal rather than simply associative. 

{\flushleft {\bf (2)}} The insight that QA benefits from task-specific embeddings. % , and a demonstration that this approach significantly improves performance. 
We implement a QA system that uses the above causal embeddings to answer questions and demonstrate that they significantly improve performance over a strong baseline. Further, we show that causal embeddings encode complementary information to vanilla embeddings, even when trained from the same knowledge resources. 

{\flushleft {\bf (3)}} An analysis of direct vs. indirect evaluations for task-specific word embeddings. 
We evaluate our causal models both  {\em directly}, in terms of measuring their capacity to rank causally-related word pairs over word pairs of other relations, as well as {\em indirectly} in the downstream causal QA task. 
%Importantly, the above knowledge acquisition process is completely independent from these evaluation tasks, e.g., the objective function of the embedding model does not include any information from the QA task, which guarantees modularity. 
In both tasks, our analysis indicates that including causal models significantly improves performance. 
However, from the direct evaluation, it is difficult to estimate which models will perform best in real-world tasks. Our analysis re-enforces recent observations about the limitations of word similarity evaluations~\cite{faruqui2016problems}: we show that they have limited coverage and may align poorly with real-world tasks.

%{\flushleft {\bf (3)}} For causal QA, we show that causal embeddings encode complementary information to vanilla embeddings, even when trained from the same knowledge resources. 

%the models that include causal embeddings perform significantly better than the models that do not. Further, for causal QA, we show that causal embeddings are complementary to vanilla embeddings, underlining the complexity of this QA task, which must simultaneously capture causality and associations driven by distributional similarity. \todo{reword? seems like we're contradicting our earlier statement...}

%{\flushleft {\bf (4)}} Finally, we show that there are discrepancies between direct and indirect evaluations, i.e., no model performs best in both tasks. Our analysis re-enforces recent observations about the limitations of narrow word similarity evaluations~\cite{faruqui2016problems}, in that they both have limited coverage, and can poorly align with real world tasks.
% end paste from emnlp2016 -------------------------


-- BUT, these shallower methods lose some of the explainability (get only a set of associations and weights on association features), so we develop approaches that focus on explanation, aggregation, and robustness

--two approaches (one more structured, one with learned representations) for answering science questions and providing compelling, human-readable explanations for the answers.

--Latent layer/intermediate output learned during training which correlates with what the model is learning

--Applied to science MCQA 

--Description (clark and jansen stuff)

--Lure answers

--To get a complete and valid explanation for selection of answer choice, may need to aggregate info from multiple distinct resources

--For aggregation, to prevent semantic drift, use structured representations (parts of CL2017)

--Robustness - shallower, no structure, learned representations (EMNLP2017-hopeful)


\section{Overview\label{sec:overview}}

chapter 2: common related work

chapter 3: naacl 2015

chapter 4: emnlp 2016 - causal

chapter 5: TAG

chapter 6: emnlp 2017 hopeful

chapter 7: discussion/conclusion