\chapter{INTRODUCTION\label{chapter:introduction}}

%\address{Start by explaining why NLI and QA are fundamental NLP tasks. Give examples to search, and personal assistants.}
\todo{Stroner opening - perhaps based on information overload and QA helping with that?} One of the primary goals for natural language processing is the development of tools that are useful for humans both at work and at home.  The classic example of one of these tools would be a search engine able to take a query and return a relevant and informative result.  In order for this application to be truly useful, though, it needs to operate over natural language queries and natural language results.  While certain queries and results can be anticipated using statistics, in order to handle a new query, the system must be able to \emph{infer} what the desired result is from the query.  This natural language inference is critical to search as well as many other types of natural language processing tasks including question answering.  

Question answering (QA), i.e., finding short answers to natural language questions, is one of the most important but challenging 
tasks on the road towards natural language understanding~\cite{Etzioni:11}. 
%\address{Require bridging lexical chasm}
As mentioned above, a QA system first faces the challenge that both the question and any potential answer sources are in the form of natural language, which inherently contains a large degree of lexical, syntactic, and even dialectal variation.
Additionally, unlike search or information retrieval, answers infrequently contain lexical overlap with the question (e.g. {\em What should we eat for breakfast? -- Zoe's Diner has good pancakes}).  This requires QA models to draw upon more complex methods to bridge this "lexical chasm" \cite{Berger:00}, i.e., to infer what the answer should be.  These methods range from robust shallow models based on lexical semantics, to deeper, explainably-correct, but much more brittle inference methods based on first order logic.  \todo{add examples of each}

\todo{high-level figure showing where my approaches fit??}
%, and is a challenging problem due to the potential complexity of the query, need to aggregate information from a variety of sources to fully answer.

%\address{Natural language inference (NLI)}
\section{Robustness}
\label{sec:robustness}

While first order logic methods are attractive for their formality, it is this same formality that renders them too brittle to be of much use outside of small, toy domains.  In order to be used, these methods require parsing natural language into formal meaning representations (a task which is very difficult given the previously mentioned language variations) and then performing inference over these representations using logic rules and axioms, again made more difficult by the open-domain (i.e., questions can be about anything and of any level of complexity) and the fact that there are never enough rules and axioms to encode everything you need to know about the world and language.  Here we focus on \emph{approximating} this formal logic inference using natural language and machine learning instead of formal representations and hand-crafted rules.  While we lose some of the ability to \emph{prove} an answer, we gain much-needed robustness to both language and domain.

Many of these shallower methods that are based on machine learning  (rather than pre-written rules) require a large amount of supervised training data, that is, data for which the label is known.  In the context of QA, these are questions for which we know the \emph{true} answer(s)\footnote{The true, or gold, answer to a question can be debatable, and there are many situations in which there can be more than one correct answer.  Typically, an answer is considered correct if a human expert labels it as such, but again, this is open to interpretation.}. In many domains, however, this requirement is expensive as large-scale, high-quality data is hard to find.  Therefore, in this work, we emphasize methods which use semi- and distant supervision\footnote{With semi-supervision, you begin with a small amount of hand-labeled data and learn to label new examples automatically, expanding your pool of labeled examples.  With distant supervision, the labeling is done automatically from the start, e.g., by aligning a database of known facts with free text instances containing portions of those facts.  For example, if you have a fact triple such as \textit{(Jane\_Smith, president\_of, College\_X)}, you can find all sentences that contain those three elements and count them all as correct instances of the relation \textit{president\_of}.  This type of approach is able to scale to large data-sources, but is very prone to noise (e.g., not all sentences truly demonstrate the \textit{president\_of} relation).}, rather than needing a large amount of hand-generated training data.  This emphasis allows these methods to be applicable across a wider variety of domains.   

Another potential problem associated with these shallower, machine learning-based methods is that with the relaxation of the formality, the explainability may also be lost, and with it the ability to understand \emph{why} the model produces the answers it does.  This model understanding is critical for being able to discern sources of model errors and fix them to attain better performance or generalization. 
%\address{why we care about explainability}
To address this, we include models (Chapters \ref{chapter:cl2017} and \ref{chapter:emnlp2017}, see also Section \ref{sec:interpretability}) that use a human-readable intermediate output generated by the model to provide an explanation for the inference performed by the model.

%\address{We try/test our NLI methods in the domain of Question answering (QA):}

% \address{QA hard bc of}

% emnlp2016 intro
%Question answering (QA), i.e., finding short answers to natural language questions, is one of the most important but challenging tasks on the road towards natural language understanding~\cite{Etzioni:11}. 
%\address{Require bridging lexical chasm}
%Unlike search or information retrieval, answers infrequently contain lexical overlap with the question (e.g. {\em What should we eat for breakfast? -- Zoe's Diner has good pancakes}), and require QA models to draw upon more complex methods to bridge this "lexical chasm" \cite{Berger:00}.  These methods range from robust shallow models based on lexical semantics, to deeper, explainably-correct, but much more brittle inference methods based on first order logic.  

%\address{One approach to bridging lexical chasm is use of monolingual alignment models, but: }

\subsection{Aligning questions and answers}
\label{sec:intro_naacl2015}
%\address{these are expensive to train (need lots of gold QA pairs)}
% naacl2015 intro
\citet{Berger:00} proposed that the "lexical chasm", or lack of lexical overlap that often exists between questions and answers, might be partially bridged by making use of techniques popular in machine translation, the task of automatically translating text from one language to another language.  At a high level, in typical machine translation, a model is given a sentence in one language aligned with its translation in another language.  The model keeps track of statistics such as how frequently words in each language co-occur in the sentence-translation pairs and uses these statistics to generate word-alignment scores.  \citeauthor{Berger:00} suggested repurposing these statistical machine translation models for QA. Instead of translating text from one language to another, these \emph{monolingual} alignment models (i.e., models that align words within the same language) learn to translate from question to answer\footnote{In practice, alignment for QA is often done from answer to question, as answers tend to be longer and provide more opportunity for association~\citep{Surdeanu:11}.}.  Given the previously mentioned example question, {\em What should we eat for breakfast? -- Zoe's Diner has good pancakes}, these models learn common associations from question terms such as {\em eat} or {\em breakfast} to answer terms like {\em kitchen, pancakes, or cereal}.

While monolingual alignment models have enjoyed a good deal of recent success in QA (see Section \ref{sec:related_work_naacl2015} for more discussion), they have expensive training data requirements,  
requiring a large set of aligned in-domain question-answer pairs for training.
In most domains these pairs are expensive to generate, and one of the current methodological challenges in QA is locating or building high-quality QA pairs for training and testing. Even large open-domain international evaluations and workshops such as the Text REtrieval Conference (TREC)\footnote{\url{http://trec.nist.gov}} and the Cross Language Evaluation Forum (CLEF),\footnote{\url{http://www.clef-initiative.eu}} are often limited to sets of a few hundred factoid questions, many of which are highly related.  As a result, for open domain QA one often makes use of Community Question Answering data from websites such as Yahoo! Answers or Stack Overflow, which offer tens of thousands of questions, but of highly variable quality.  
For low-resource languages or specialized domains like science or biology, often the only option is to enlist a domain expert to generate gold QA pairs --  a process that is both expensive and time consuming.  All of this means that only in rare cases are we accorded the luxury of having enough high-quality QA pairs to properly train an alignment model, and so these models are often underutilized or left struggling for resources. 

%\address{We propose a method for generating artificial training data (can be thought of as a form of distant supervision?? If you squint?) (NAACL2015)}
% naacl2015 intro
Making use of recent advancements in discourse parsing \citep{feng12}, in Chapter \ref{chapter:naacl2015} we address this issue, and investigate whether alignment models for QA can be trained from \emph{artificial} question-answer pairs generated from discourse structures imposed on free text. This method can be thought of as a form of distant supervision, as we don't have true labels for which free-text pairs are truly high-quality proxies for question-answer pairs.
% by imposing structure on inexpensive free text resources instead of using QA pairs.  
We evaluate our methods on two corpora, generating alignment models for an open-domain community QA task using Gigaword\footnote{LDC catalog number LDC2012T21}, and for a biology-domain QA task using a biology textbook. 


\subsection{Customizing approaches to the question type}
\label{sec:intro_emnlp2016}
%\address{Diff info needs and so the “manner” by which the lexical chasm is bridged should hopefully be robust to that}

This alignment approach for QA can be considered as falling into a larger group of approaches which prefer answers that are closely related to the question, where the relatedness is determined by the associations of the alignment model (e.g., as briefly referred to in Section \ref{sec:intro_naacl2015}) or by associations provided by other lexical semantic models such as word embeddings~\citep{yih13,jansen14,fried2015higher}. 
While appealing for its robustness to natural language variation, this one-size-fits-all category of approaches does not take into account the wide range of distinct question types that can appear in any given question set (see, e.g.,  Table \ref{tab:inferenceexamples}), and that are best addressed individually~\citep{chu2004ibm,ferrucci2010building,clark2013study} for a specific question set.  

%These don’t address diff types of inference/info needs:
%\address{We propose a framework for learning customized alignments/associations for a specific info need using semi-supervised methods (EMNLP2016)}
Given the variety of question types, we suggest that a better approach is to look for answers that are related to the question \emph{through the appropriate relation}, e.g., a causal question should have a cause-effect relation with its answer.
Adopting this view, and working with embeddings as a mechanism for assessing relationship, in Chapter \ref{chapter:emnlp2016} we address a key question: how do we train and use task-specific embeddings cost-effectively? 
Using causality as a use case, we answer this question with a framework for producing causal word embeddings with minimal supervision, and a demonstration that such task-specific embeddings significantly benefit causal QA. 

\section{Interpretability}
\label{sec:interpretability}

%\address{BUT, these shallower methods lose some of the explainability (get only a set of associations and weights on association features), so we develop approaches that focus on explanation, aggregation, and robustness}

%\address{two approaches (one more structured, one with learned representations) for answering science questions and providing compelling, human-readable explanations for the answers.}

%\address{Latent layer/intermediate output learned during training which correlates with what the model is learning}

% ------ pasted from emnlp2017-------------
%\todo{I don't think this text really fits here at all - perhaps move back to the paper/chapter intro and write something else for here}

Developing interpretable machine learning models, that is, models where a human user can \emph{understand} what the model is learning, is considered by many to be crucial for ensuring usability and accelerating progress \citep{craven1996extracting,Kim2015MindTG, letham2015interpretable, Ribeiro2016WhySI}.  
% bs: removed for space, we talk more about this in related work
%As such, it has received much attention in recent years, especially as deep learning and complex architectures have seen dramatic gains in many tasks.
For many applications of question answering (QA), simply providing an answer (or even an answer alongside a list of word-associations and their corresponding model weights, as with the approaches briefly described in Section \ref{sec:robustness}) is not sufficient. A complete approach must be interpretable, i.e., able to {\em explain} why an answer is correct. 
%We believe that justifying why the QA algorithm believes an answer is correct is, in many cases, a critical part of the QA process.
For example, in the medical domain, a user would not trust a system that recommends invasive procedures without giving a justification as to why (e.g., ``Smith (2005) found procedure \emph{X} healed 90\% of patients with heart disease who also had secondary pulmonary complications'').  A QA tool is clearly more useful when its human user can identify both when it functions correctly, and when it delivers an incorrect or misleading result -- especially in situations where incorrect results carry a high cost.  
%For example, in the medical domain, a QA approach that answers treatment questions would not be trusted if the treatment recommendation is not explained in terms that can be understood by the human user. 

One approach to interpreting complex models is to make use of human-interpretable information generated by the model to gain insight into what the model is learning.  This can be an intermediate representation used by the model, as with the model-generated text spans of \citet{Lei2016RationalizingNP}, that serve as input to another classification network.  
By learning these intermediate representations end-to-end with a downstream task (i.e., question answering or another task that the user is interested in), they are optimized to correlate with what the model learns is discriminatory for the task, and they can be evaluated against what a human would consider to be important.  

\subsection{Multiple choice science questions as a proving ground}
\label{sec:mcqa}

\begin{table}[t]
\begin{center}
\begin{tabularx}{\linewidth}{p{1cm}p{13cm}}
\multicolumn{2}{p{15cm}}{\textbf{Question:} Which of these is a response to an internal stimulus?} \\
 (A) & A sunflower turns to face the rising sun. \\
 (B) & A cucumber tendril wraps around a wire. \\
 (C) &  A pine tree knocked sideways in a landslide grows upward in a bend. \\
 (\textbf{D}) &\textbf{Guard cells of a tomato plant leaf close when there is little water in the roots .} \\
\\
\multicolumn{2}{p{15cm}}{\textbf{Justification:} 
Plants rely on hormones to send signals within the plant in order to respond to internal stimuli such as a lack of water or nutrients. } \\
\end{tabularx}

\caption{{  Example of an 8th grade science question with a justification for the correct answer.  Note the lack of direct lexical overlap present between the justification and the correct answer, demonstrating the difficulty of the task of finding justifications using traditional distant supervision methods. }}

\label{tab:question_example}
\end{center}
\end{table}

\begin{table*}[t]
\begin{center}
\small
\begin{tabularx}{\textwidth}{p{2.5cm}p{5.2cm}p{5.9cm}}
\hline
Category &	\multicolumn{2}{l}{Example} \\
\hline
Retrieval	&	\multicolumn{2}{l}{Q: The movement of soil by wind or water is called:} \\
(35\%)		&   (A) condensation   	&	(B) evaporation   \\
			&	(C) erosion   		&	(D) friction \\
\\
General 	&	\multicolumn{2}{l}{Q: Which example describes an organism taking in nutrients?} \\
Inference	&   (A) A dog burying a bone			&   (B) A girl eating an apple	\\
(39\%)		&	(C) An insect crawling on a leaf	&  (D) A boy planting tomatoes in the garden  \\
\\
Model-based & 	\multicolumn{2}{l}{Q: When a baby shakes a rattle, it makes a noise. Which form of} \\
Inference	& 	\multicolumn{2}{l}{energy was changed to sound energy?} \\
(26\%)		&	(A) electrical	&   (B) light   \\
			&	(C) mechanical	&   (D) heat  \\
			
\end{tabularx}
\caption{ 
Categories of questions and their relative frequencies as identified by \citet{clark:2013}. Retrieval-based questions (including \emph{is--a}, dictionary definition, and property identification questions) tend to be answerable using information retrieval methods over structured knowledge bases, including taxonomies and dictionaries. 
More complex general inference questions make use of either simple inference rules that apply to a particular situation, a knowledge of causality, or a knowledge of simple processes (such as \emph{solids melt when heated}).
Difficult model-based reasoning questions require a domain-specific model of how a process works, like how gravity causes planets to orbit stars, in order to be correctly answered.
Note here that we do not include diagram questions, as they require specialized spatial reasoning that is beyond the scope of this work. }
\label{tab:inferenceexamples}
\end{center}
\end{table*}

In Chapters \ref{chapter:cl2017} and \ref{chapter:emnlp2017}, we apply this general framework for model interpretability to QA, and in particular to answering multiple-choice science exam questions \citep{clark:2015}.  An example science exam question is provided in Table~\ref{tab:question_example} to demonstrate the role of the justification in explaining the correct answer choice as well as to illustrate the fact that retrieving such a justification is non-trivial due to lack of lexical overlap. 
%Science exams are an important proving ground for QA because inference is often required to arrive at a correct answer, and, commonly, incorrect answers that are high semantic associates of either the question or correct answer are included to ``lure'' students (or automated methods) away from the correct response.

In addition to the general difficulties of QA previously described, this particular QA domain is challenging as approximately 70\% of science exam questions have been shown to require complex forms of inference to solve \citep{clark:2013,jansen-EtAl:2016:COLING}.  In Table \ref{tab:inferenceexamples} we provide example questions from each of the three main categories of questions from \citet{clark:2013}, where the categories are based on the methods likely required to answer them correctly.  Not only do the majority of questions require some form of inference to solve, but there are few structured knowledge bases to support this inference, and also commonly incorrect answers that are high semantic associates of either the question or correct answer are included to ``lure'' students (or automated methods) away from the correct response.

\subsection{Reranking justifications with weak supervision}
\label{sec:reranking_justifications}

Within the domain of multiple-choice science QA, we propose two approaches that reframe QA from the task of scoring (or reranking) answers to a process of \emph{generating and evaluating justifications} for why a particular answer candidate is correct.  The first approach (Chapter \ref{chapter:cl2017}) uses aggregation of information from several sources along with structured representations of the text to create justifications of the answer choices.  The second (Chapter \ref{chapter:emnlp2017}) uses a shallower approach without aggregation or structured representations, allowing it to operate over larger text resources.  Each of these approaches learn to both select and explain answers, when the only supervision available is for which answer is correct (but not how to explain it).
Intuitively, our approaches choose the justifications that provide the most help towards ranking the correct answers higher than incorrect ones.
More formally, our approaches alternate between using the current model to choose the highest scoring justifications for answers, and optimizing the answer ranking model given these justifications. 
Crucially, these reranked texts serve as our human-readable answer justifications, and by examining them, we gain insight into what the model learned was useful for the QA task. 


%------end paste from emnlp 2017--------------------------


%\address{Applied to science MCQA }

%\address{Description (clark and jansen stuff)}

%\address{Lure answers}

%To encourage a emphasis on the task of explainable inference for question answering (QA), \citet{clark:2015} introduced the Aristo challenge, a QA task focused on developing methods of automated inference capable of passing standardized elementary school science exams, while also providing human-readable explanations (or justifications) for those answers.  Science exams are an important proving ground for QA because inference is often required to arrive at a correct answer, and, commonly, incorrect answers that are high semantic associates of either the question or correct answer are included to ``lure'' students (or automated methods) away from the correct response.
%
%  In spite of being multiple choice, these questions tend to be  more challenging than factoid QA, which is highly amenable to retrieval-based models that operate over large structured knowledge bases such as Freebase (e.g. \note{Liang?, etc}). \todo{The previous sentence must be defended.} Multiple choice exams also commonly contain "lure" answers -- incorrect answers that are high semantic associates of either the question or correct answer -- which further reduce the efficacy of retrieval or lexical semantic/associative methods. 
%- Semantic Parsing for QA / Freebase (Percy Liang), much easier problem
%
%-- Science exams as a challenging QA problem
%- More than just factoid lookup, despite being multiple choice
%- Interesting proving ground for QA -- questions are challenging, and well-constructed multiple-choice questions typically have lure answers that are incorrect but are highly associated with either the question or correct answer, making shallow methods difficult (REF). 
%

%Adding to the challenge, not only is inference required for science exams, but several kinds of inference are present.
%In an analysis of three years of standardized science exams, \citet{clark:2013} identified three main categories of questions based on the methods likely required to answer them correctly. Examples of these questions can be seen in Table~\ref{tab:inferenceexamples}, highlighting that 65\% of questions require some form of inference to be answered.
%
%\begin{table*}[t]
%\caption{ 
%Categories of questions and their relative frequencies as identified by \citet{clark:2013}. Retrieval-based questions (including \emph{is--a}, dictionary definition, and property identification questions) tend to be answerable using information retrieval methods over structured knowledge bases, including taxonomies and dictionaries. 
%More complex general inference questions make use of either simple inference rules that apply to a particular situation, a knowledge of causality, or a knowledge of simple processes (such as \emph{solids melt when heated}).
%Difficult model-based reasoning questions require a domain-specific model of how a process works, like how gravity causes planets to orbit stars, in order to be correctly answered.
%Note here that we do not include diagram questions, as they require specialized spatial reasoning that is beyond the scope of this work. 
%}
%\begin{center}
%\small
%\begin{tabularx}{\textwidth}{p{2cm}p{5cm}p{5.9cm}}
%\hline
%Category &	\multicolumn{2}{l}{Example} \\
%\hline
%Retrieval	&	\multicolumn{2}{l}{Q: The movement of soil by wind or water is called:} \\
%(35\%)		&   (A) condensation   	&	(B) evaporation   \\
%			&	(C) erosion   		&	(D) friction \\
%\\
%General 	&	\multicolumn{2}{l}{Q: Which example describes an organism taking in nutrients?} \\
%Inference	&   (A) A dog burying a bone			&   (B) A girl eating an apple	\\
%(39\%)		&	(C) An insect crawling on a leaf	&  (D) A boy planting tomatoes in the garden  \\
%\\
%Model-based & 	\multicolumn{2}{l}{Q: When a baby shakes a rattle, it makes a noise. Which form of energy was} \\
%Inference	& 	\multicolumn{2}{l}{changed to sound energy?} \\
%(26\%)		&	(A) electrical	&   (B) light   \\
%			&	(C) mechanical	&   (D) heat  \\
%			
%\end{tabularx}
%\label{tab:inferenceexamples}
%\end{center}
%\end{table*}
%
%
%%-- Approximate Inference/Continuum
%%- benefits/drawbacks
%%- Alignment/Lexical semantic end: Robust but lacks justifications
%%- First-order Logic end: Brittle, but provably correct justifications
%%- Meeting in the middle (relax FOL, or impose more structure into LS)
%
%
%%-- Justifications as central to question answering
%%- In many applications, answering without justifications is pointless
%%- Example (medical -- you need surgery, but not explain why)
%%- Model QA as generating and evaluating explanations/justifications for why a particular answer candidate is correct
%
%
%
%%-- Reframing QA as a generating and evaluating explanations/justifications for why a particular answer candidate is correct
%
%To address these issues, here we reframe QA from the task of scoring (or reranking) answers to 
%a process of \emph{generating and evaluating justifications} for why a particular answer candidate is correct. 
%We focus on answering science exam questions, where many questions require complex inference, and building and evaluating answer justifications is challenging. 

%\address{To get a complete and valid explanation for selection of answer choice, may need to aggregate info from multiple distinct resources}

%\address{For aggregation, to prevent semantic drift, use structured representations (parts of CL2017)}
%In particular, we construct justifications by {\em aggregating} multiple sentences from a number of textual knowledge bases (e.g., study guides, science dictionaries), which, in combination, aim to explain the answer.
%We then rank these candidate justifications based on a number of measures designed to assess how well integrated, relevant, and on-topic a given justification is, and select the answer that corresponds to the highest-ranked justification.

%\address{Robustness - shallower, no structure, learned representations (EMNLP2017-hopeful)}


% ----------CONTRIBUTIONS-
\section{Contributions}
\label{sec:Contributions}

The specific contributions of this work are:


% naacl 2015
\subsection{Contribution 1: Using discourse structures to generate artificially aligned pairs for training question answering models} 
We demonstrate that by exploiting the discourse structure of free text, monolingual alignment models can be trained to surpass the performance of models built from expensive in-domain question-answer pairs.  To this end, we compare two methods of discourse parsing: a simple sequential model, and a deep model based on Rhetorical Structure Theory (RST)~\citep{mann88} and show that the RST-based method captures within and across-sentence alignments and performs better than the sequential model, but the sequential model is an acceptable approximation when a discourse parser is not available.  The proposed methods are evaluated on two corpora, including a low-resource domain where training data is expensive (biology). We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-the-art neural network language models in low resource domains.

% emnlp 2016
\subsection{Contribution 2: Generating customized, task-specific word embeddings based on the question type} We propose a methodology for generating causal word-embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns, e.g., {\em X causes Y}. 
We then train dedicated word embedding (as well as two other distributional similarity) models over this data. \citet{levy2014dependency} have modified the algorithm of\citet{mikolov2013distributed} to use an arbitrary, rather than linear, context. Here we make this context \emph{task-specific}, i.e., the context of a cause is its effect.
Further, to mitigate sparsity and noise, our models are bidirectional, and noise-aware (by incorporating the likelihood of noise in the training process). 
We implement a QA system that uses these causal embeddings to answer questions and demonstrate that they significantly improve performance over a strong baseline. Further, we show that causal embeddings encode complementary information to the standard (or \emph{vanilla}) word-embeddings, even when trained from the same knowledge resources. 
We also analyze direct vs. indirect evaluations for task-specific word embeddings -- evaluating our causal models both  {\em directly}, in terms of measuring their capacity to rank causally-related word pairs over word pairs of other relations, as well as {\em indirectly} in the downstream causal QA task. 
In both tasks, our analysis indicates that including causal models significantly improves performance. 
However, from the direct evaluation, it is difficult to estimate which models will perform best in real-world tasks. Our analysis re-enforces recent observations about the limitations of word similarity evaluations~\citep{faruqui2016problems}: we show that they have limited coverage and may align poorly with real-world tasks.

% cl2017
\subsection{Contribution 3: Creating and ranking justifications for interpretable question answering} We propose a method to construct answer justifications through information aggregation. 
In particular, we aggregate multiple sentences into hierarchical graph structures (called text aggregation graphs) that capture both intrasentence syntactic structures and intersentence lexical overlaps. 
Further, we model whether the intersentence lexical overlap is between contextually relevant keywords critical to the justification, or other words which may or may not be relevant. 
Our empirical analysis demonstrates that modeling the contextual relevance of intersentence connections is crucial for good performance.  %These requirements highlight the fundamental differences between selecting a single answer sentence or short passage in an answer sentence selection task~\cite[inter alia]{Severyn:12,Severyn:13a,Severyn:13b}, and the task of generating complete answer justifications through information aggregation. 
This approach uses a latent-variable ranking perceptron algorithm that learns to jointly rank answers and justifications, where the quality of justifications is modeled as the latent variable. 
We evaluate our system on a large corpus of 1,000 elementary science exam questions from third to fifth grade, and demonstrate that our system significantly outperforms several strong learning-to-rank baselines at the task of choosing the correct answer.  Further, we manually annotate answer justifications provided by the best baseline model and our intersentence aggregation method, and show that the intersentence aggregation method produces good justifications for 57\% of questions answered correctly, significantly outperforming the best baseline method. 
Through an in-depth error analysis, we show that most of the issues encountered by the intersentence aggregation method center on solvable surface issues rather than complex inference issues.  To our knowledge, this is the largest evaluation and most in-depth error analysis for explainable inference in the context of elementary science exams. 


% conll 2017
\subsection{Contribution 5: Using neural networks to rank justifications for interpretable question answering} We propose an end-to-end neural method for learning to answer questions and select a high-quality justification for those answers. 
This approach re-ranks free-text answer justifications \emph{without} the need for structured knowledge bases. 
With supervision only for the correct answers, we learn this re-ranking through a form of distant supervision -- i.e., the answer ranking supervises the justification re-ranking. In this approach, we investigate two distinct categories of features in this ``little data'' domain: explicit features, and learned representations. We show that, with limited training, explicit features perform far better despite their simplicity.  We also demonstrate a large (+9\%) improvement in generating high-quality justifications over a strong information retrieval baseline, while maintaining near state-of-the-art performance on the multiple-choice science-exam QA task, demonstrating the success of the end-to-end strategy.



% end paste from cl2017 --------------------------

\section{Overview\label{sec:overview}}

The rest of this work is organized as follows.  In Chapter \ref{chapter:related_work} we outline the previous work relevant to our task, as well as how it relates to each of our approaches.  
We then detail our word-association approaches to question answering that emphasize robustness: in Chapter \ref{chapter:naacl2015} we present our alignment approach whereby we generate artifically aligned texts to serve as a proxy for aligned question-answer pairs; then in Chapter \ref{chapter:emnlp2016} we present our method for tailoring word-associations (here, in the form of word-embeddings) to a specific question-type.  The next two chapters contain our methods which focus more on model-interpretability, providing a human-readable justification for each answer selected by the model.  In Chapter \ref{chapter:cl2017} we detail an approach that uses aggregation to generate justifications for answers, extracts features based on a structured representation of the aggregated justiifcation, and then uses these features in a latent-variable reranking perceptron.  The perceptron learns to simultaneously rank these justifications and answer candidates, thus providing the answer as well as the human-readable justification.  In Chapter \ref{chapter:emnlp2017} we modify this approach to operate over much larger resources by removing the aggregation component as well as the structured representations, meanwhile extending the learning framework to a non-linear neural network.  Finally, in Chapter \ref{chapter:conclusion} we discuss the work as a whole as well as future directions.
