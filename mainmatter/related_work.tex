\chapter{RELATED WORK\label{chapter:related_work}}

In one sense, QA systems can be described in terms of their position along a formality continuum ranging from shallow models that rely on information retrieval, lexical semantics, or alignment, to highly structured models based on first order logic (as depicted in Figure \todo{make a figure for intro}).

On the shallower end of the spectrum,  QA models can be constructed either from structured text, such as question--answer pairs, or unstructured text.  These models are largely based on finding patterns in word association scores, which serve as a proxy for formal inference.  For example, using one of the questions from Table \ref{tab:inferenceexamples}, \emph{Q: Which example describes an organism taking in nutrients? A: A girl eating an apple}, rather than knowing specifically that \emph{girl} is an organism and \emph{eating} is a manner of taking in nutrients, the model would look for a higher association between the question words \emph{organism, taking} and \emph{nutrients} and the correct answer words \emph{girl, eating} and \emph{apple} than between those same question words and words from an incorrect answer, such as \emph{insect, crawling} and \emph{leaf}.  Understandably, these methods struggle with more complex inference and with lure answers which are close associates with the correct answer. 

Within this category of association-based model, monolingual alignment models~\citep{Berger:00,echihabi2003noisy,Soricut:06,Riezler:etal:2007,Surdeanu:11,yao2013} have been widely employed.  These models utilize statistical machine translation techniques to learn \textit{translations} not from one language to another, but rather from question words to words likely to be found in their answers.  These alignment-based models, however, like their machine translation counterparts, require a large number of aligned text pairs for training.  In the case of the alignment models, this training data consists of aligned question--answer pairs, a burden which often limits their practical usage.  In Chapter \ref{chapter:naacl2015} we address this burden by proposing a method for using the discourse structure of free text as a surrogate for this alignment structure. %\todo{ref to a related work section in that chapter? or keep it here in a subsection?}

Lexical semantic models such as neural-network language models~\citep{jansen14,sultan-etal:2014:TACL,yih13}, on the other hand, have the advantage of being readily constructed from free text.  These models use distributional similarity via high-dimensional dense word embeddings to create sets of similarity features.  These features are intended to capture how related a question is to a given answer candidate.  
\citet{fried2015higher} called these approaches first-order models because associations are explicitly learned for words that co-occur in text.  In this way, if two words never co-occur, an association would never be \emph{directly} learned\footnote{While two words which never occur together would never have an association directly learned by the model, indirect learning takes place (which is, in a large part, what makes these models so robust).  That is, during the training process, words which occur with similar context words are indirectly drawn closer together in the high-dimensional embedded space as they are each independently drawn closer to their overlapping context words.  For example, words like \emph{dog} and \emph{platypus} will each be independently drawn closer to a common context word like \emph{eat}, inevitably resulting in them being indirectly drawn closer to each other.}.   To achieve a form of approximated inference based on chaining together associations, they introduced a higher-order lexical semantics QA model where indirect associations are detected through traversals of the association graph.  For example, the directly learned associations between word pairs like \textit{virus} $\leftrightarrow$ \textit{infection} and \textit{infection} $\leftrightarrow$ \textit{fever} would serve to form or strengthen the transitive association between \textit{virus} $\leftrightarrow$ \textit{fever}. 
%Other recent efforts have applied deep learning architectures to QA to learn non-linear answer scoring functions that model lexical semantics~\citep{Iyyer2014,nips15_hermann}.

These alignment and lexical semantic approaches, however do not take into account the wide variety of question types which often exist in any given question set.  Therefore, they attempt to answer \emph{all} questions with word-pair associations from the same set of standard (similarity-based) word embeddings.  To address this, we propose an approach in Chapter \ref{chapter:emnlp2016} for training a dedicated set of word embeddings that are customized to a particular semantic relation (here, causality) and demonstrate that the semantic information contained in these customized embeddings is complementary to that which is contained in standard word embeddings and useful for answering questions of the corresponding type (again, here we address \textit{causal} questions, though the method is readily extendable to other question types).

While these shallower (i.e., alignment and lexical semantic) approaches to QA have shown robust performance across a variety of tasks, one continuing disadvantage of these methods is that, even when a correct answer is selected, there is no clear human-readable justification for that selection.  This limits our ability to effectively understand model performance and adjust for errors accordingly.

Closer to the other end of the formality continuum, several approaches were proposed to not only select a correct answer, but also provide a formally valid justification for that answer.  For example, some QA systems have sought to answer questions by creating formal proofs driven by logic reasoning over sets of semantic rules~\citep[e.g.,][]{moldovan2003cogex,moldovan2007cogex,
balduccini2008knowledge,
maccartney2009natural,liang2013learning,
lewis2013combining}.
Some formal systems have made use of answer-set programming \citep{baral2006using,baral2011towards,baral2012answering,
baral2012knowledge} to computationally search through a set of answers to find one for which a formal proof (from question to answer) can be constructed. 
Still others have constructed semantic graph representations (based on semantic role information) of sentences and questions, attempting to resolve missing roles to find the answer~\citep[e.g.,][]{banarescu2012amr,sharmatowards}. 
However, the formal representations used in these systems, e.g., logic forms or semantic graphs, are both expensive to generate and tend to be brittle because they rely extensively on imperfect tools for unsolved problems (such as complete syntactic analysis and word sense disambiguation) as well as incomplete knowledge bases and rule sets.  %As a practical example, the semantic graphs rely on databases of the semantic roles of known verbs.  The coverage of these databases, however, is limited both in domain as well as language and so the systems that build upon them are as well. 

In Chapter \ref{chapter:cl2017}, we offer a lightly-structured graph-based sentence representation (see Section \ref{sec-cl2017:tag}) as a shallower and consequently more robust approximation of those logical forms.  In Section \ref{sec-cl2017:results} we show that they are well-suited for the complex questions (see Section \ref{sec:mcqa}) we tackle.
This approach allows us to robustly aggregate information from a variety of knowledge sources to create human-readable answer justifications.  
It is these justifications which we then rank in order to choose the correct answer, using a reranking perceptron extended with a latent layer that models the correctness of those justifications.

While this approach is shallower than many of the approaches based on formal representations, it still requires decomposing free text resources into lightly-structured representations and performing a somewhat expensive aggregation step.  In practice, this limits its use with extremely large text corpora.  Additionally, as the learning framework is an extension of a linear perceptron, it is inherently limited by its assumption of the data being linearly-separable within the feature space.  To address these limitations, in Chapter \ref{chapter:emnlp2017} we propose a shallower version of this model.  This new model uses a non-linear neural network architecture to learn a task-specific embedded representation of each of the question, answer, and candidate justification texts.  These representations are then used alongside a small set of explicit features to re-rank the candidate justifications, given the question and answer.  By removing the graph-based representation, we increase the robustness, allowing us to use the model with much larger text corpora.   Additionally, by continuing to re-rank the candidate justifications, we maintain the model interpretability -- since the top $n$ justifications returned for each chosen answer provide insight into what the model learned was important for finding correct answers.

In both of these latter approaches, the way we have formulated our justification selection (as a re-ranking of knowledge base sentences) is related to, but yet distinct from the task of answer sentence selection \cite[][inter alia]{Wang2010ProbabilisticTM, Severyn:12,Severyn:13a,Severyn:13b,Severyn2015LearningTR,wang2015long}.  Answer sentence selection is typically framed as a fully or semi-supervised task for factoid questions (i.e., questions whose answers are limited to one or more facts, such as birth-dates or names), where the goal of the task is to correctly select a sentence that from a corpus that fully contains the answer text.  For example, to answer the question \textit{What is the capital of France?}, a system would attempt to select a sentence such as \textit{In the French capital of Paris, ...}.
Our QA task, however, is unlike answer-sentence selection.  Here, we have a variety of questions, many of which are non-factoid (i.e., questions whose answers are not facts, such as those based on \textit{how} or \textit{why} something happens).  Additionally, we have no direct supervision for our justification selection (i.e., no labels as to which sentences are good justifications for our answers), motivating our distant supervision approach where the performance on our QA task serves as supervision for selecting good justifications.  Further, we are not actually looking for sentences that \emph{contain} the answer choice, as with answer sentence selection, but rather sentences which close the "lexical chasm" \citep{Berger:00} between question and answer (demonstrated in the example in Table \ref{tab:question_example}).  That is, we are seeking as justifications sentences which provide the missing step in the \emph{inference} needed to answer the question.
