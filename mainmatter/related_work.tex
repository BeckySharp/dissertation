\chapter{RELATED WORK\label{chapter:related_work}}

In one sense, QA systems can be described in terms of their position along a formality continuum ranging from shallow models that rely on information retrieval, lexical semantics, or alignment, to highly structured models based on first order logic (as depicted in Figure \todo{make a figure for intro}).

On the shallower end of the spectrum,  QA models can be constructed either from structured text, such as question--answer pairs, or unstructured text.  These models are largely based on finding patterns in word association scores, which serve as a proxy for formal inference.  For example, using one of the questions from Table \ref{tab:inferenceexamples}, \emph{Q: Which example describes an organism taking in nutrients? A: A girl eating an apple}, rather than knowing specifically that \emph{girl} is an organism and \emph{eating} is a manner of taking in nutrients, the model would look for a higher association between the question words \emph{organism, taking} and \emph{nutrients} and the correct answer words \emph{girl, eating} and \emph{apple} than between those same question words and words from an incorrect answer, such as \emph{insect, crawling} and \emph{leaf}.  Understandably, these methods struggle with more complex inference and with lure answers which are close associates with the correct answer. 

Within this category of association-based model, monolingual alignment models~\citep{Berger:00,echihabi2003noisy,Soricut:06,Riezler:etal:2007,Surdeanu:11,yao2013}  require aligned question--answer pairs for training, a burden which often limits their practical usage.  In Chapter \ref{chapter:naacl2015} we address this burden by proposing a method for using the discourse structure of free text as a surrogate for this alignment structure. %\todo{ref to a related work section in that chapter? or keep it here in a subsection?}

Lexical semantic models such as neural-network language models~\citep{jansen14,sultan-etal:2014:TACL,yih13}, which make use of high-dimensional dense embeddings for words that are designed to encode distributional similarity, on the other hand, have the advantage of being readily constructed from free text.  
\citet{fried2015higher} called these approaches first-order models because associations are explicitly learned, and introduced a higher-order lexical semantics QA model where indirect associations are detected through traversals of the association graph.  
Other recent efforts have applied deep learning architectures to QA to learn non-linear answer scoring functions that model lexical semantics~\citep{Iyyer2014,nips15_hermann}.

These alignment and lexical semantic approaches, however do not take into account the wide variety of question types which often exist in any given question set.  Therefore, they attempt to answer \emph{all} questions with word-pair associations from the same set of standard (similarity-based) word embeddings.  To address this, we propose an approach in Chapter \ref{chapter:emnlp2016} for training a dedicated set of word embeddings that are customized to a particular semantic relation (here, causality) and demonstrate that the semantic information contained in these customized embeddings is complementary to that which is contained in standard word embeddings and useful in a causal QA task.

While these shallower (i.e., alignment and lexical semantic) approaches to QA have shown robust performance across a variety of tasks, one continuing disadvantage of these methods is that, even when a correct answer is selected, there is no clear human-readable justification for that selection.  This limits our ability to effectively understand model performance and adjust for errors accordingly.

Closer to the other end of the formality continuum, several approaches were proposed to not only select a correct answer, but also provide a formally valid justification for that answer.  For example, some QA systems have sought to answer questions by creating formal proofs driven by logic reasoning~\citep{moldovan2003cogex,moldovan2007cogex,balduccini2008knowledge,maccartney2009natural,liang2013learning,lewis2013combining}, answer-set programming \citep{baral2006using,baral2011towards,baral2012answering,baral2012knowledge}, or connecting semantic graphs~\citep{banarescu2012amr,sharmatowards}. 
However, the formal representations used in these systems, e.g., logic forms, are both expensive to generate and tend to be brittle because they rely extensively on imperfect tools for unsolved problems such as complete syntactic analysis and word sense disambiguation.

In Chapter \ref{chapter:cl2017}, we offer a lightly-structured graph-based sentence representation (see Section \ref{sec-cl2017:tag}) as a shallower and consequently more robust approximation of those logical forms.  In Section \ref{sec-cl2017:results} we show that they are well-suited for the complex questions (see Section \ref{sec:mcqa}) we tackle.
This approach allows us to robustly aggregate information from a variety of knowledge sources to create human-readable answer justifications.  
It is these justifications which we then rank in order to choose the correct answer, using a reranking perceptron extended with a latent layer that models the correctness of those justifications.

While this approach is shallower than many of the approaches based on formal representations, it still requires decomposing free text resources into lightly-structured representations and performing a somewhat expensive aggregation step.  In practice, this limits its use with extremely large text corpora.  Additionally, the perceptron-based learning framework is inherently limited by its assumption of the data being linearly-separable.  To address these, in Chapter \ref{chapter:emnlp2017} we propose a shallower version of this model that can learn an embedded representation of the question, answer, and candidate justification texts and that can re-rank justifications using these representations alongside a small set of explicit features.  In this way, we maintain the human-readable justifications (i.e., the interpretability) while increasing the robustness.

In both of these latter approaches, the way we have formulated our justification selection (as a re-ranking of knowledge base sentences) is related to, but yet distinct from the task of answer sentence selection \cite[][inter alia]{Wang2010ProbabilisticTM, Severyn:12,Severyn:13a,Severyn:13b,Severyn2015LearningTR,wang2015long}.  Answer sentence selection is typically framed as a fully or semi-supervised task for factoid questions, where a correctly selected sentence fully contains the answer text.
Here, we have a variety of questions, many of which are non-factoid.  Additionally, we have no direct supervision for our justification selection (i.e., no labels as to which sentences are good justifications for our answers), motivating our distant supervision approach where the performance on our QA task serves as supervision for selecting good justifications.  Further, we are not actually looking for sentences that \emph{contain} the answer choice, as with answer sentence selection, but rather sentences which close the "lexical chasm" \citep{Berger:00} between question and answer (demonstrated in the example in Table \ref{tab:question_example}).  That is, we are seeking as justifications sentences which provide the missing step in the \emph{inference} needed to answer the question.
