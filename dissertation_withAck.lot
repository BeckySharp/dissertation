\addvspace {10\p@ }
\contentsline {table}{\numberline {1.1}{\ignorespaces { Examples of questions from Yahoo! Answers, a community question answering website, that demonstrate the variable quality of the community-chosen \textit {best} answer. }}}{21}{table.1.1}
\contentsline {table}{\numberline {1.2}{\ignorespaces { Example of an 8th grade science question with a justification for the correct answer that completes the needed inference between question and answer. Note the lack of direct lexical overlap present between the justification and the correct answer, demonstrating the difficulty of the task of finding justifications using traditional distant supervision methods. }}}{25}{table.1.2}
\contentsline {table}{\numberline {1.3}{\ignorespaces Categories of questions and their relative frequencies as identified by \citet {clark:2013}. Retrieval-based questions (including \emph {is--a}, dictionary definition, and property identification questions) tend to be answerable using information retrieval methods over structured knowledge bases, including taxonomies and dictionaries. More complex general inference questions make use of either simple inference rules that apply to a particular situation, a knowledge of causality, or a knowledge of simple processes (such as \emph {solids melt when heated}). Difficult model-based reasoning questions require a domain-specific model of how a process works, like how gravity causes planets to orbit stars, in order to be correctly answered. Note here that we do not include diagram questions, as they require specialized spatial reasoning that is beyond the scope of this work. }}{26}{table.1.3}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces The 18 discourse relation labels that can be assigned by the Rhetorical Structure Theory (RST) parser of \citet {Surdeanu:15}. }}{41}{table.3.1}
\contentsline {table}{\numberline {3.2}{\ignorespaces Feature descriptions for alignment models and the embedding baseline.}}{42}{table.3.2}
\contentsline {table}{\numberline {3.3}{\ignorespaces {Comparison of performance for the discourse model when all discourse relations are used versus when only the top 6 most frequent (\textit {elaboration, attribution, background, contrast, joint, and same-unit}) are used.}}}{48}{table.3.3}
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces {Number of causal tuples extracted from each corpus.}}}{57}{table.4.1}
\contentsline {table}{\numberline {4.2}{\ignorespaces {Performance in the QA evaluation, measured by precision-at-one (P@1). The ``Bi'' suffix indicates a bidirectional model; the ``Noise'' suffix indicates a model that is noise aware. $^*$ indicates that the difference between the corresponding model and the IR + vEmbed baseline is statistically significant ($p < 0.05$), as determined through a one-tailed bootstrap resampling test with 10,000 iterations. }}}{69}{table.4.2}
\contentsline {table}{\numberline {4.3}{\ignorespaces {SVM weights learned for each of the features in the combination model IR + vEmbed + cEmbedBi. Recall that feature values themselves are all independently normalized to lie between 0.0 and 1.0.}}}{71}{table.4.3}
\contentsline {table}{\numberline {4.4}{\ignorespaces {Results of an error analysis performed on a random sample of 20 incorrectly answered questions showing the source of the error and the percentage of questions that were affected. Note that questions can belong to multiple categories. }}}{71}{table.4.4}
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces { Example of an elementary science question with a justification constructed by our approach (in this case, each sentence comes from a different dictionary resource). Note that while each sentence is relevant to the inference required to answer the question, neither is sufficient without the other. When combined, however, the sentences complete the necessary inference. }}}{75}{table.5.1}
\contentsline {table}{\numberline {5.2}{\ignorespaces {Focus word decomposition of an example question, suggesting the question is primarily about measuring the speed of walking, and not about turtles or paths. (Correct answer: ``a stopwatch and meter stick.'') For a given word: \emph {Conc} refers to the psycholinguistic concreteness score, \emph {Tag} refers to the focus word category (\emph {FOCUS} signifies a focus word, \emph {EX} an example word, \emph {ATYPE} an answer-type word, and \emph {ST} a stop word), \emph {Score} refers to the focus word score, and \emph {Weight} refers to the normalized focus word scores. }}}{79}{table.5.2}
\contentsline {table}{\numberline {5.3}{\ignorespaces { Features used to score candidate answer justifications represented as TAGs. }}}{87}{table.5.3}
\contentsline {table}{\numberline {5.4}{\ignorespaces { Performance as a function of justification length in sentences (or, the number of graphlets in a TAG) for two models: one aware of connection-type, and one that is not. Bold font indicates the best score in a given column for each model group. }}}{98}{table.5.4}
\contentsline {table}{\numberline {5.5}{\ignorespaces { Performance of the baseline and best-performing TAG models, both separately and in combination. TAG justifications of different short lengths were found to best combine in single classifiers (denoted with a $+$), where models that combine the IR baseline or long (3G) TAG justifications best combined using voting ensembles (denoted with a $\cup $). Bold font indicates the best score in a given column for each model group. Asterisks indicate that a score is significantly better than the highest-performing baseline model (* signifies $p < 0.05$, ** signifies $p < 0.01$). The dagger indicates that a score is significantly higher than the score in the line number indicated in superscript ($p < 0.01$). All significance tests were implemented using one-tailed non-parametric bootstrap resampling using 10,000 iterations. }}}{100}{table.5.5}
\contentsline {table}{\numberline {5.6}{\ignorespaces { Example justifications from the IR baseline and their associated ratings. }}}{101}{table.5.6}
\contentsline {table}{\numberline {5.7}{\ignorespaces {An example TAG and justification rated as {\em good}. The two sentences connect on non-focus "other" shared words (e.g., \emph {green, plant}) which are not found in the question or answer, but which are highly related to the focus words. }}}{102}{table.5.7}
\contentsline {table}{\numberline {5.8}{\ignorespaces { \emph {At least one} justification performance for both IR and TAG models, reflecting the highest rating attained by at least one of the top six justifications for a given question. }}}{103}{table.5.8}
\contentsline {table}{\numberline {5.9}{\ignorespaces { Most useful knowledge resources for justifications classified as "good".}}}{106}{table.5.9}
\contentsline {table}{\numberline {5.10}{\ignorespaces { Proportion of good justifications with a given number of connecting word categories (Q, A, X) for both correct and incorrect answers. (Top 6) }}}{108}{table.5.10}
\contentsline {table}{\numberline {5.11}{\ignorespaces { A summary of the inference type necessary for incorrectly answered questions. The summary is broken down into three categories: incorrectly answered questions with a good justification in the top six, incorrectly answered questions without a good justification in the top six, as well as the overall proportions across these two conditions. }}}{109}{table.5.11}
\contentsline {table}{\numberline {5.12}{\ignorespaces { A summary of the classes of the errors made by the system. On any given question, more than one error may have been made. The summary is broken down into three categories: incorrectly answered questions with a good justification in the top six, incorrectly answered questions without a good justification in the top six, as well as the overall proportions across these two conditions.}}}{110}{table.5.12}
\contentsline {table}{\numberline {5.13}{\ignorespaces { Example of failure to extract appropriate focus words from the question. }}}{111}{table.5.13}
\contentsline {table}{\numberline {5.14}{\ignorespaces { Example of a question that needs more than two sentences to answer. }}}{113}{table.5.14}
\contentsline {table}{\numberline {5.15}{\ignorespaces { Example of a question that requires reasoning over a causal structure or process. }}}{114}{table.5.15}
\contentsline {table}{\numberline {5.16}{\ignorespaces { Example of a question that requires an understanding of the quantifiers in both the question and the answers. }}}{114}{table.5.16}
\contentsline {table}{\numberline {5.17}{\ignorespaces { Example of a question that requires an understanding of negation. }}}{115}{table.5.17}
\contentsline {table}{\numberline {5.18}{\ignorespaces { Example of a failure to recognize relatedness or equivalence of words. }}}{116}{table.5.18}
\addvspace {10\p@ }
\contentsline {table}{\numberline {6.1}{\ignorespaces { Summary of the features calculated for each candidate justification. }}}{125}{table.6.1}
\contentsline {table}{\numberline {6.2}{\ignorespaces { Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1). $^*$s indicate that the difference between the corresponding model and the IR baseline is statistically significant ($^*$ indicates $p < 0.05$ and $^{**}$ indicates $p < 0.001$) and $^{\dagger }$s indicate significance compared to IR$^{++}$, All significance values were determined through a one-tailed bootstrap resampling test with 100,000 iterations. }}}{132}{table.6.2}
\contentsline {table}{\numberline {6.3}{\ignorespaces { Ablation of feature groups results, measured by precision-at-one (P@1) on validation data. Emb indicates our embedding-based features, LO indicates our lexical overlap features, lexDisc signifies our semi-lexicalized discourse features, and IR$^{++}$ indicates our information retrieval based features. Significance is indicated as in Table \ref {tab:p@1}.}}}{133}{table.6.3}
\contentsline {table}{\numberline {6.4}{\ignorespaces { Example justifications from the our model and their associated ratings. }}}{136}{table.6.4}
\contentsline {table}{\numberline {6.5}{\ignorespaces {Percentage of questions that have at least one \emph {good} justification within the top 1 (Good@1) and the top 5 (Good@5) justifications, as well as the normalized discounted cumulative gain at 5 (NDCG@5) of the ranked justifications. Significance indicated as in Table \ref {tab:p@1}. }}}{137}{table.6.5}
\contentsline {table}{\numberline {6.6}{\ignorespaces { Summary of the findings of the 30 question error analysis. Examples of several categories are provided in separate tables. Note that a given question may fall into more than one category.}}}{139}{table.6.6}
\contentsline {table}{\numberline {6.7}{\ignorespaces { Example of the system preferring a justification for which all the terms were found in either the question or answer candidate, while the justification for the correct answer contained additional information necessary to fully explain the answer. (Justifications shown in italics) }}}{139}{table.6.7}
\contentsline {table}{\numberline {6.8}{\ignorespaces { Example of a question for which complex inference is required. In order to answer the question, you would need to assemble the event chain: cut grass left on the ground $\rightarrow $ grass decomposes $\rightarrow $ decomposed material provides nutrients.}}}{140}{table.6.8}
\contentsline {table}{\numberline {6.9}{\ignorespaces { Example of a question for which knowledge base noise (here, in the form of over-generalization) was an issue.}}}{140}{table.6.9}
\addvspace {10\p@ }
\addvspace {10\p@ }
