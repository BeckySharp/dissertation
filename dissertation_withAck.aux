\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{LIST OF FIGURES}{9}{chapter*.6}}
\citation{clark:2013}
\citation{Surdeanu:15}
\@writefile{toc}{\contentsline {chapter}{LIST OF TABLES}{12}{chapter*.7}}
\@writefile{toc}{\contentsline {chapter}{ABSTRACT}{16}{chapter*.8}}
\@input{mainmatter/introduction.aux}
\@input{mainmatter/related_work.aux}
\citation{Berger:00}
\citation{echihabi2003noisy,Soricut:06,Riezler:etal:2007,Surdeanu:11,yao2013}
\citation{jansen14,sultan-etal:2014:TACL,yih13}
\@writefile{toc}{\contentsline {chapter}{CHAPTER \numberline {3}Using Free Text to Train Monolingual Alignment Models for Question Answering }{38}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:naacl2015}{{3}{38}{Using Free Text to Train Monolingual Alignment Models for Question Answering}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Chapter overview}{38}{section.3.1}}
\citation{Verberne:2007}
\citation{mann88}
\citation{jansen14}
\citation{marcu97}
\citation{hickl2006recognizing}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related Work}{39}{section.3.2}}
\newlabel{sec-naacl2015:relatedwork}{{3.2}{39}{Related Work}{section.3.2}{}}
\citation{halliday2014cohesion}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces {\relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip An example of the alignments produced by the two discourse models. The sequential model aligns pairs of consecutive sentences, capturing intersentence associations such as \emph  {cider--apples}, and \emph  {orchard--autumn}. The Rhetorical Structure Theory based model generates alignment pairs from participants in all (binary) discourse relations, capturing both intrasentence and intersentence alignments, including \emph  {apples--orchard, cider--apples}, and \emph  {cider--autumn}.}}}{40}{figure.3.1}}
\newlabel{fig:examples}{{3.1}{40}{\small An example of the alignments produced by the two discourse models. The sequential model aligns pairs of consecutive sentences, capturing intersentence associations such as \emph {cider--apples}, and \emph {orchard--autumn}. The Rhetorical Structure Theory based model generates alignment pairs from participants in all (binary) discourse relations, capturing both intrasentence and intersentence alignments, including \emph {apples--orchard, cider--apples}, and \emph {cider--autumn}}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Approach}{40}{section.3.3}}
\newlabel{sec-naacl2015:approach}{{3.3}{40}{Approach}{section.3.3}{}}
\citation{Surdeanu:15}
\citation{hernault10}
\citation{feng12}
\citation{Surdeanu:15}
\citation{Surdeanu:15}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces   The 18 discourse relation labels that can be assigned by the Rhetorical Structure Theory (RST) parser of \citet  {Surdeanu:15}. }}{41}{table.3.1}}
\newlabel{tab:rst}{{3.1}{41}{The 18 discourse relation labels that can be assigned by the Rhetorical Structure Theory (RST) parser of \citet {Surdeanu:15}}{table.3.1}{}}
\citation{Brown:93}
\citation{fried2015higher}
\citation{jansen14}
\citation{jansen14}
\citation{manning08}
\citation{mikolov10,mikolov13}
\citation{yih13}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Feature descriptions for alignment models and the embedding baseline.}}{42}{table.3.2}}
\newlabel{tab:Features}{{3.2}{42}{Feature descriptions for alignment models and the embedding baseline}{table.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Models and Features}{42}{section.3.4}}
\newlabel{sec-naacl2015:models}{{3.4}{42}{Models and Features}{section.3.4}{}}
\citation{Brown:93}
\citation{och03}
\citation{Surdeanu:11}
\citation{Surdeanu:11}
\citation{Surdeanu:11}
\citation{fried2015higher}
\citation{mikolov13}
\citation{jansen14}
\citation{Reece:2011}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experiments}{44}{section.3.5}}
\newlabel{sec-naacl2015:experiments}{{3.5}{44}{Experiments}{section.3.5}{}}
\citation{Napoles2012}
\citation{manning08}
\citation{jansen14}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Results and Discussion}{45}{section.3.6}}
\newlabel{sec-naacl2015:results}{{3.6}{45}{Results and Discussion}{section.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{How does the performance of the Rhetorical Structure Theory (RST) and sequential (SEQ) models compare?}{45}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces {\relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Overall performance for the two discourse-based alignment models (the Rhetorical Structure Theory based model and the sequential model), compared against the IR baseline, random baseline, and an embedding-based reranker. The $x$ axis indicates the number of training documents used to construct all models. Each point represents the average of 10 samples of training documents. }}}{46}{figure.3.2}}
\newlabel{fig:performance}{{3.2}{46}{\small Overall performance for the two discourse-based alignment models (the Rhetorical Structure Theory based model and the sequential model), compared against the IR baseline, random baseline, and an embedding-based reranker. The $x$ axis indicates the number of training documents used to construct all models. Each point represents the average of 10 samples of training documents}{figure.3.2}{}}
\newlabel{footnote:bootstrap}{{11}{46}{}{Hfootnote.20}{}}
\citation{fried2015higher}
\citation{jansen14}
\@writefile{toc}{\contentsline {paragraph}{How does the performance of the RST model compare to a model trained on in-domain question-answer pairs?}{47}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{How does the performance of the RST model compare to previous work?}{47}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{How do the RST and SEQ models compare to the non-alignment baselines?}{47}{section*.12}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces {Comparison of performance for the discourse model when all discourse relations are used versus when only the top 6 most frequent (\textit  {elaboration, attribution, background, contrast, joint, and same-unit}) are used.}}}{48}{table.3.3}}
\newlabel{tab:ablation}{{3.3}{48}{Comparison of performance for the discourse model when all discourse relations are used versus when only the top 6 most frequent (\textit {elaboration, attribution, background, contrast, joint, and same-unit}) are used}{table.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{How does the SEQ model compare to the random baseline?}{48}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{What is the contribution of the individual discourse relations used in generating the alignment pairs?}{48}{table.3.3}}
\@writefile{toc}{\contentsline {paragraph}{Why does performance plateau in YA and not in Bio?}{49}{section*.15}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Conclusion}{49}{section.3.7}}
\newlabel{sec-naacl2015:conclusion}{{3.7}{49}{Conclusion}{section.3.7}{}}
\citation{jansen-EtAl:2016:COLING}
\citation{oh2013question}
\@writefile{toc}{\contentsline {chapter}{CHAPTER \numberline {4}Creating Causal Embeddings for Question Answering \\with Minimal Supervision }{51}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:emnlp2016}{{4}{51}{Creating Causal Embeddings for Question Answering \\with Minimal Supervision}{chapter.4}{}}
\citation{fried2015higher,yih13}
\citation{levy2015supervised}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter overview}{52}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Related Work}{52}{section.4.2}}
\newlabel{sec-emnlp2016:relatedwork}{{4.2}{52}{Related Work}{section.4.2}{}}
\citation{fitzgerald2015semantic,woodsenddistributed}
\citation{riedel2013relation}
\citation{riedel2013relation}
\citation{bordes2014question}
\citation{yang2014joint}
\citation{levy2014dependency,kielaspecializing}
\citation{faruqui2016problems}
\citation{girju2002text}
\citation{hearst1992automatic}
\citation{cole2005lightweight}
\citation{yang2014multi}
\citation{do2011minimally}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Chapter overview}{54}{section.4.3}}
\citation{sharp-EtAl:2015:NAACL-HLT}
\citation{valenzuela2016runes}
\citation{Cunningham2011a}
\citation{chambers2007learning}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Extracting Cause-Effect Tuples}{55}{section.4.4}}
\newlabel{sec-emnlp2016:causalextraction}{{4.4}{55}{Extracting Cause-Effect Tuples}{section.4.4}{}}
\citation{napoles2012annotated}
\citation{Manning:14}
\citation{chen14}
\citation{khoo1998automatic}
\newlabel{step:cm}{{4.4}{56}{Extracting Cause-Effect Tuples}{section.4.4}{}}
\newlabel{step:causalext}{{4.4}{56}{Extracting Cause-Effect Tuples}{section.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces {Number of causal tuples extracted from each corpus.}}}{57}{table.4.1}}
\newlabel{tab:causalstats}{{4.1}{57}{Number of causal tuples extracted from each corpus}{table.4.1}{}}
\citation{mikolov2013distributed}
\citation{yih13,fried2015higher}
\citation{levy2014dependency}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Models}{58}{section.4.5}}
\newlabel{sec-emnlp2016:models}{{4.5}{58}{Models}{section.4.5}{}}
\citation{Berger:00,Echihabi:03,Soricut:06,Riezler:etal:2007,Surdeanu:11,yao2013}
\citation{sharp-EtAl:2015:NAACL-HLT}
\citation{Brown:93}
\citation{och03}
\citation{iyyer2015deep}
\citation{kingma2014adam}
\citation{chollet2015keras}
\citation{2016arXiv160502688short}
\citation{hochreiter1997long}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces { Architecture of the causal convolutional network. }}}{59}{figure.4.1}}
\newlabel{fig:cnn}{{4.1}{59}{ Architecture of the causal convolutional network}{figure.4.1}{}}
\citation{riloff1996automatically}
\citation{levy2014dependency}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Direct Evaluation: Ranking Word Pairs}{61}{section.4.6}}
\newlabel{sec-emnlp2016:directeval}{{4.6}{61}{Direct Evaluation: Ranking Word Pairs}{section.4.6}{}}
\citation{hendrickx2009semeval}
\citation{hendrickx2009semeval}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Data}{62}{subsection.4.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Baselines}{62}{subsection.4.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces {Precision-recall curve showing the ability of each model to rank causal pairs above non-causal pairs. The Look-up model has no data points beyond the 35\% recall point.}}}{63}{figure.4.2}}
\newlabel{fig:rpcurve_all}{{4.2}{63}{Precision-recall curve showing the ability of each model to rank causal pairs above non-causal pairs. The Look-up model has no data points beyond the 35\% recall point}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Results}{63}{subsection.4.6.3}}
\newlabel{sec-emnlp2016:results}{{4.6.3}{63}{Results}{subsection.4.6.3}{}}
\citation{curran2007minimising}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces {Illustration of the indirect learning of associations that facilitates approximate inference. Note that in this illustrated example, the learned associations are noisy, but this mechanism is critical -- it is what allows embedding models to generalize beyond the explicit training examples. The blue highlight indicates words embedded in the cause (target) space, and the yellow indicates words embedded in the effect (context) space.}}}{64}{figure.4.3}}
\newlabel{fig:inference}{{4.3}{64}{Illustration of the indirect learning of associations that facilitates approximate inference. Note that in this illustrated example, the learned associations are noisy, but this mechanism is critical -- it is what allows embedding models to generalize beyond the explicit training examples. The blue highlight indicates words embedded in the cause (target) space, and the yellow indicates words embedded in the effect (context) space}{figure.4.3}{}}
\citation{faruqui2016problems}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces {Precision-recall curve including the bidirectional variants of the causal models. For clarity, we do not plot the basic noise aware causal embedding model (cEmbedNoise), which performs worse than its bidirectional variant (cEmbedBiNoise, described in Section \ref  {sec-emnlp2016:results}). }}}{66}{figure.4.4}}
\newlabel{fig:rpcurve_withBi}{{4.4}{66}{Precision-recall curve including the bidirectional variants of the causal models. For clarity, we do not plot the basic noise aware causal embedding model (cEmbedNoise), which performs worse than its bidirectional variant (cEmbedBiNoise, described in Section \ref {sec-emnlp2016:results})}{figure.4.4}{}}
\citation{jansen14}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Indirect Evaluation: QA Task}{67}{section.4.7}}
\newlabel{sec-emnlp2016:indirecteval}{{4.7}{67}{Indirect Evaluation: QA Task}{section.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Data}{67}{subsection.4.7.1}}
\citation{fried2015higher}
\citation{Surdeanu:11}
\citation{fried2015higher}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Models and Features}{68}{subsection.4.7.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces {Performance in the QA evaluation, measured by precision-at-one (P@1). The ``Bi'' suffix indicates a bidirectional model; the ``Noise'' suffix indicates a model that is noise aware. $^*$ indicates that the difference between the corresponding model and the IR + vEmbed baseline is statistically significant ($p < 0.05$), as determined through a one-tailed bootstrap resampling test with 10,000 iterations. }}}{69}{table.4.2}}
\newlabel{tab:QA}{{4.2}{69}{Performance in the QA evaluation, measured by precision-at-one (P@1). The ``Bi'' suffix indicates a bidirectional model; the ``Noise'' suffix indicates a model that is noise aware. $^*$ indicates that the difference between the corresponding model and the IR + vEmbed baseline is statistically significant ($p < 0.05$), as determined through a one-tailed bootstrap resampling test with 10,000 iterations}{table.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Results}{70}{subsection.4.7.3}}
\newlabel{sec-emnlp2016:results}{{4.7.3}{70}{Results}{subsection.4.7.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces {SVM weights learned for each of the features in the combination model IR + vEmbed + cEmbedBi. Recall that feature values themselves are all independently normalized to lie between 0.0 and 1.0.}}}{71}{table.4.3}}
\newlabel{tab:weights}{{4.3}{71}{SVM weights learned for each of the features in the combination model IR + vEmbed + cEmbedBi. Recall that feature values themselves are all independently normalized to lie between 0.0 and 1.0}{table.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces {Results of an error analysis performed on a random sample of 20 incorrectly answered questions showing the source of the error and the percentage of questions that were affected. Note that questions can belong to multiple categories. }}}{71}{table.4.4}}
\newlabel{tab:ea}{{4.4}{71}{Results of an error analysis performed on a random sample of 20 incorrectly answered questions showing the source of the error and the percentage of questions that were affected. Note that questions can belong to multiple categories}{table.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Error Analysis}{72}{subsection.4.7.4}}
\newlabel{sec-emnlp2016:erroranalysis}{{4.7.4}{72}{Error Analysis}{subsection.4.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Conclusion}{72}{section.4.8}}
\newlabel{sec-emnlp2016:conclusion}{{4.8}{72}{Conclusion}{section.4.8}{}}
\@writefile{toc}{\contentsline {chapter}{CHAPTER \numberline {5}Interpretable Question Answering: Building and Ranking Intersentence Answer Justifications }{74}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:cl2017}{{5}{74}{Interpretable Question Answering: Building and Ranking Intersentence Answer Justifications}{chapter.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces { Example of an elementary science question with a justification constructed by our approach (in this case, each sentence comes from a different dictionary resource). Note that while each sentence is relevant to the inference required to answer the question, neither is sufficient without the other. When combined, however, the sentences complete the necessary inference. }}}{75}{table.5.1}}
\newlabel{tab:question_and_justification}{{5.1}{75}{ Example of an elementary science question with a justification constructed by our approach (in this case, each sentence comes from a different dictionary resource). Note that while each sentence is relevant to the inference required to answer the question, neither is sufficient without the other. When combined, however, the sentences complete the necessary inference}{table.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Chapter Outline}{75}{section.5.1}}
\citation{barzilay1999information,barzilay2005sentence}
\citation{pradhan2002building}
\citation{blair2003hybrid}
\citation{Moldovan:2003:PIE:763693.763694}
\citation{Harabagiu:00,Moldovan:2003:PIE:763693.763694}
\citation{Park:2015}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Related Work}{76}{section.5.2}}
\newlabel{sec-cl2017:relatedwork}{{5.2}{76}{Related Work}{section.5.2}{}}
\citation{Shen:Joshi:2005}
\citation{Surdeanu:11}
\citation{liang2006end,zettlemoyer2007online,sun2009latent,hoffmann2011knowledge,fernandes2012latent,bjorkelund2014learning}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Our QA approach, which centers on the construction of answer justifications as text aggregation graphs, and ranking them using a model that treats the justification quality as a latent variable.}}{77}{figure.5.1}}
\newlabel{fig:architecture}{{5.1}{77}{Our QA approach, which centers on the construction of answer justifications as text aggregation graphs, and ranking them using a model that treats the justification quality as a latent variable}{figure.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Approach}{77}{section.5.3}}
\newlabel{sec-cl2017:approach}{{5.3}{77}{Approach}{section.5.3}{}}
\citation{jansen2017framing}
\citation{Piaget1954}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces {Focus word decomposition of an example question, suggesting the question is primarily about measuring the speed of walking, and not about turtles or paths. (Correct answer: ``a stopwatch and meter stick.'') For a given word: \emph  {Conc} refers to the psycholinguistic concreteness score, \emph  {Tag} refers to the focus word category (\emph  {FOCUS} signifies a focus word, \emph  {EX} an example word, \emph  {ATYPE} an answer-type word, and \emph  {ST} a stop word), \emph  {Score} refers to the focus word score, and \emph  {Weight} refers to the normalized focus word scores. }}}{79}{table.5.2}}
\newlabel{tab:focusexample}{{5.2}{79}{Focus word decomposition of an example question, suggesting the question is primarily about measuring the speed of walking, and not about turtles or paths. (Correct answer: ``a stopwatch and meter stick.'') For a given word: \emph {Conc} refers to the psycholinguistic concreteness score, \emph {Tag} refers to the focus word category (\emph {FOCUS} signifies a focus word, \emph {EX} an example word, \emph {ATYPE} an answer-type word, and \emph {ST} a stop word), \emph {Score} refers to the focus word score, and \emph {Weight} refers to the normalized focus word scores}{table.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Focus Word Extraction}{79}{section.5.4}}
\newlabel{sec-cl2017:focuswords}{{5.4}{79}{Focus Word Extraction}{section.5.4}{}}
\citation{brysbaert:2014}
\citation{jansen2017framing}
\citation{de2008stanford}
\citation{brysbaert:2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Scores and weights}{81}{subsection.5.4.1}}
\citation{jansen2017framing}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Text Aggregation Graphs}{82}{section.5.5}}
\newlabel{sec-cl2017:tag}{{5.5}{82}{Text Aggregation Graphs}{section.5.5}{}}
\citation{Voorhees:2003}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Five example graphlets for five sentences that could be aggregated together in different combinations to justifiably answer the question \emph  {What tools could determine the speed of turtles walking along a path? (Answer: stopwatch and meter stick)}. Each graphlet contains one or more information nuggets (grey boxes) composed of one or more terms. For example, the graphlet for the sentence \emph  {A stopwatch can be used to measure time} contains two information nuggets. Edges between nuggets within a graphlet are shown with arrows, where a subset of these edges are labelled (e.g., \emph  {EXAMPLE, INSTRUMENT}), and the rest are unlabelled. }}{83}{figure.5.2}}
\newlabel{fig:conceptexamples}{{5.2}{83}{Five example graphlets for five sentences that could be aggregated together in different combinations to justifiably answer the question \emph {What tools could determine the speed of turtles walking along a path? (Answer: stopwatch and meter stick)}. Each graphlet contains one or more information nuggets (grey boxes) composed of one or more terms. For example, the graphlet for the sentence \emph {A stopwatch can be used to measure time} contains two information nuggets. Edges between nuggets within a graphlet are shown with arrows, where a subset of these edges are labelled (e.g., \emph {EXAMPLE, INSTRUMENT}), and the rest are unlabelled}{figure.5.2}{}}
\citation{manning2014stanford}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces  Two example Text Aggregation Graphs (TAGs) that justifiably answer the question \emph  {What tools could determine the speed of turtles walking along a path} for the answer \emph  {a stopwatch and meter stick}. Asterisks (*) denote that a given term is either a question or answer focus word, while pounds (\#) denote terms that are not found in the question or answer, but which are shared between graphlets. Links between the graphlets in a given TAG are highlighted. (top) A two-sentence TAG, where the edges between graphlets connect on a focus word \emph  {(speed)} and other words shared between the graphlets \emph  {(distance, measure)}. (bottom) A three-sentence TAG, where the edges between graphlets connect entirely on shared words between the graphlets \emph  {(distance, measure, time)} that are not focus words. }}{85}{figure.5.3}}
\newlabel{fig:tag_example}{{5.3}{85}{Two example Text Aggregation Graphs (TAGs) that justifiably answer the question \emph {What tools could determine the speed of turtles walking along a path} for the answer \emph {a stopwatch and meter stick}. Asterisks (*) denote that a given term is either a question or answer focus word, while pounds (\#) denote terms that are not found in the question or answer, but which are shared between graphlets. Links between the graphlets in a given TAG are highlighted. (top) A two-sentence TAG, where the edges between graphlets connect on a focus word \emph {(speed)} and other words shared between the graphlets \emph {(distance, measure)}. (bottom) A three-sentence TAG, where the edges between graphlets connect entirely on shared words between the graphlets \emph {(distance, measure, time)} that are not focus words}{figure.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Text Aggregation Graph Features}{86}{section.5.6}}
\newlabel{sec-cl2017:scoring}{{5.6}{86}{Text Aggregation Graph Features}{section.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Features}{86}{subsection.5.6.1}}
\newlabel{sec-cl2017:featuresandscoring}{{5.6.1}{86}{Features}{subsection.5.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces { Features used to score candidate answer justifications represented as TAGs. }}}{87}{table.5.3}}
\newlabel{tab:features}{{5.3}{87}{ Features used to score candidate answer justifications represented as TAGs}{table.5.3}{}}
\citation{Berger:00}
\citation{jansen14}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces {Connections between sentences are characterized based on lexical overlap between graphlets. Here, each box represents a two-sentence TAG, with graphlets stacked vertically. The presence of question or answer focus words is marked with \emph  {Q} or \emph  {A}, while the presence of other non-focus words shared between the two graphlets is marked with \emph  {X}. Lexical overlap within a category is highlighted. }}}{88}{figure.5.4}}
\newlabel{fig:connectiontypes}{{5.4}{88}{Connections between sentences are characterized based on lexical overlap between graphlets. Here, each box represents a two-sentence TAG, with graphlets stacked vertically. The presence of question or answer focus words is marked with \emph {Q} or \emph {A}, while the presence of other non-focus words shared between the two graphlets is marked with \emph {X}. Lexical overlap within a category is highlighted}{figure.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Modeling Different TAG Types Using Domain Adaptation}{88}{subsection.5.6.2}}
\newlabel{sec-cl2017:characterizing}{{5.6.2}{88}{Modeling Different TAG Types Using Domain Adaptation}{subsection.5.6.2}{}}
\citation{daume2007}
\citation{finkel2010hierarchical}
\citation{finkel2010hierarchical}
\citation{Shen:Joshi:2005,Surdeanu:11}
\citation{hoffmann2011knowledge}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Learning Model}{90}{section.5.7}}
\newlabel{sec-cl2017:perceptron}{{5.7}{90}{Learning Model}{section.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Learning Algorithm}{91}{subsection.5.7.1}}
\newlabel{sec-cl2017:model}{{5.7.1}{91}{Learning Algorithm}{subsection.5.7.1}{}}
\newlabel{eq:F}{{5.1}{91}{Learning Algorithm}{equation.5.7.1}{}}
\citation{Shen:Joshi:2005}
\citation{hoffmann2011knowledge}
\citation{Collins:2002:DTM}
\citation{Surdeanu:11}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  Learning algorithm for the latent reranking perceptron. We consider, without loss of generality, that the correct answer appears at position 1 in training.}}{93}{algorithm.1}}
\newlabel{alg:perceptron}{{1}{93}{Learning Algorithm}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Experiments}{93}{section.5.8}}
\newlabel{sec-cl2017:experiments}{{5.8}{93}{Experiments}{section.5.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}Data}{93}{subsection.5.8.1}}
\newlabel{sec-cl2017:data}{{5.8.1}{93}{Data}{subsection.5.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}Tuning}{94}{subsection.5.8.2}}
\newlabel{sec-cl2017:tuning}{{5.8.2}{94}{Tuning}{subsection.5.8.2}{}}
\citation{jansen14}
\citation{manning08}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.3}Baselines}{95}{subsection.5.8.3}}
\newlabel{sec-cl2017:baselines}{{5.8.3}{95}{Baselines}{subsection.5.8.3}{}}
\citation{Shen:Joshi:2005,Surdeanu:11}
\citation{jansen14}
\citation{mikolov13,mikolov10}
\citation{manning08}
\citation{mcsherry2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.4}Results}{97}{subsection.5.8.4}}
\newlabel{sec-cl2017:results}{{5.8.4}{97}{Results}{subsection.5.8.4}{}}
\citation{fried2015higher}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces { Performance as a function of justification length in sentences (or, the number of graphlets in a TAG) for two models: one aware of connection-type, and one that is not. Bold font indicates the best score in a given column for each model group. }}}{98}{table.5.4}}
\newlabel{tab:pathlength}{{5.4}{98}{ Performance as a function of justification length in sentences (or, the number of graphlets in a TAG) for two models: one aware of connection-type, and one that is not. Bold font indicates the best score in a given column for each model group}{table.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Justification Length}{98}{section*.16}}
\newlabel{sec-cl2017:pathlength}{{5.8.4}{98}{Justification Length}{section*.16}{}}
\citation{fried2015higher}
\citation{daume2007}
\citation{fried2015higher}
\citation{jansen14}
\citation{jansen14}
\citation{sharp-EtAl:2015:NAACL-HLT}
\@writefile{toc}{\contentsline {subsubsection}{Combined Models}{99}{section*.17}}
\newlabel{sec-cl2017:combinedmodels}{{5.8.4}{99}{Combined Models}{section*.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces { Performance of the baseline and best-performing TAG models, both separately and in combination. TAG justifications of different short lengths were found to best combine in single classifiers (denoted with a $+$), where models that combine the IR baseline or long (3G) TAG justifications best combined using voting ensembles (denoted with a $\cup $). Bold font indicates the best score in a given column for each model group. Asterisks indicate that a score is significantly better than the highest-performing baseline model (* signifies $p < 0.05$, ** signifies $p < 0.01$). The dagger indicates that a score is significantly higher than the score in the line number indicated in superscript ($p < 0.01$). All significance tests were implemented using one-tailed non-parametric bootstrap resampling using 10,000 iterations. }}}{100}{table.5.5}}
\newlabel{tab:combinedmodels}{{5.5}{100}{ Performance of the baseline and best-performing TAG models, both separately and in combination. TAG justifications of different short lengths were found to best combine in single classifiers (denoted with a $+$), where models that combine the IR baseline or long (3G) TAG justifications best combined using voting ensembles (denoted with a $\cup $). Bold font indicates the best score in a given column for each model group. Asterisks indicate that a score is significantly better than the highest-performing baseline model (* signifies $p < 0.05$, ** signifies $p < 0.01$). The dagger indicates that a score is significantly higher than the score in the line number indicated in superscript ($p < 0.01$). All significance tests were implemented using one-tailed non-parametric bootstrap resampling using 10,000 iterations}{table.5.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces { Example justifications from the IR baseline and their associated ratings. }}}{101}{table.5.6}}
\newlabel{tab:justificationsIRexamples}{{5.6}{101}{ Example justifications from the IR baseline and their associated ratings}{table.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Justifications}{101}{section*.18}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces {An example TAG and justification rated as {\em  good}. The two sentences connect on non-focus "other" shared words (e.g., \emph  {green, plant}) which are not found in the question or answer, but which are highly related to the focus words. }}}{102}{table.5.7}}
\newlabel{ex:tagQsXs}{{5.7}{102}{An example TAG and justification rated as {\em good}. The two sentences connect on non-focus "other" shared words (e.g., \emph {green, plant}) which are not found in the question or answer, but which are highly related to the focus words}{table.5.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces { \emph  {At least one} justification performance for both IR and TAG models, reflecting the highest rating attained by at least one of the top six justifications for a given question. }}}{103}{table.5.8}}
\newlabel{tab:justifications}{{5.8}{103}{ \emph {At least one} justification performance for both IR and TAG models, reflecting the highest rating attained by at least one of the top six justifications for a given question}{table.5.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Ablation Studies}{103}{section*.19}}
\newlabel{sec-cl2017:controls}{{5.8.4}{103}{Ablation Studies}{section*.19}{}}
\citation{Khashabi2016QuestionAV}
\citation{Khashabi2016QuestionAV}
\citation{Khashabi2016QuestionAV}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Discussion}{105}{section.5.9}}
\newlabel{sec-cl2017:discussion}{{5.9}{105}{Discussion}{section.5.9}{}}
\citation{fried2015higher}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces { Most useful knowledge resources for justifications classified as "good".}}}{106}{table.5.9}}
\newlabel{tab:justificationknowledgeresources}{{5.9}{106}{ Most useful knowledge resources for justifications classified as "good"}{table.5.9}{}}
\newlabel{tab:errorconnectiontypes}{{5.10}{108}{Error Analysis}{section.5.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces { Proportion of good justifications with a given number of connecting word categories (Q, A, X) for both correct and incorrect answers. (Top 6) }}}{108}{table.5.10}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Error Analysis}{108}{section.5.10}}
\newlabel{sec-cl2017:erroranalysis}{{5.10}{108}{Error Analysis}{section.5.10}{}}
\citation{clark:2013}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces { A summary of the inference type necessary for incorrectly answered questions. The summary is broken down into three categories: incorrectly answered questions with a good justification in the top six, incorrectly answered questions without a good justification in the top six, as well as the overall proportions across these two conditions. }}}{109}{table.5.11}}
\newlabel{tab:knowledgetype}{{5.11}{109}{ A summary of the inference type necessary for incorrectly answered questions. The summary is broken down into three categories: incorrectly answered questions with a good justification in the top six, incorrectly answered questions without a good justification in the top six, as well as the overall proportions across these two conditions}{table.5.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.12}{\ignorespaces { A summary of the classes of the errors made by the system. On any given question, more than one error may have been made. The summary is broken down into three categories: incorrectly answered questions with a good justification in the top six, incorrectly answered questions without a good justification in the top six, as well as the overall proportions across these two conditions.}}}{110}{table.5.12}}
\newlabel{tab:errorclasses}{{5.12}{110}{ A summary of the classes of the errors made by the system. On any given question, more than one error may have been made. The summary is broken down into three categories: incorrectly answered questions with a good justification in the top six, incorrectly answered questions without a good justification in the top six, as well as the overall proportions across these two conditions}{table.5.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.13}{\ignorespaces { Example of failure to extract appropriate focus words from the question. }}}{111}{table.5.13}}
\newlabel{ex:majorfw}{{5.13}{111}{ Example of failure to extract appropriate focus words from the question}{table.5.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.1}Broader Error Classes}{111}{subsection.5.10.1}}
\citation{jansen-EtAl:2016:COLING}
\@writefile{lot}{\contentsline {table}{\numberline {5.14}{\ignorespaces { Example of a question that needs more than two sentences to answer. }}}{113}{table.5.14}}
\newlabel{ex:longerchains}{{5.14}{113}{ Example of a question that needs more than two sentences to answer}{table.5.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.15}{\ignorespaces { Example of a question that requires reasoning over a causal structure or process. }}}{114}{table.5.15}}
\newlabel{ex:structure}{{5.15}{114}{ Example of a question that requires reasoning over a causal structure or process}{table.5.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.16}{\ignorespaces { Example of a question that requires an understanding of the quantifiers in both the question and the answers. }}}{114}{table.5.16}}
\newlabel{ex:quantifiers}{{5.16}{114}{ Example of a question that requires an understanding of the quantifiers in both the question and the answers}{table.5.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.17}{\ignorespaces { Example of a question that requires an understanding of negation. }}}{115}{table.5.17}}
\newlabel{ex:negations}{{5.17}{115}{ Example of a question that requires an understanding of negation}{table.5.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.18}{\ignorespaces { Example of a failure to recognize relatedness or equivalence of words. }}}{116}{table.5.18}}
\newlabel{ex:clustermatching}{{5.18}{116}{ Example of a failure to recognize relatedness or equivalence of words}{table.5.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.2}Summary of Errors}{117}{subsection.5.10.2}}
\citation{moldovan2001logic}
\@writefile{toc}{\contentsline {section}{\numberline {5.11}Conclusion}{118}{section.5.11}}
\newlabel{sec-cl2017:conclusion}{{5.11}{118}{Conclusion}{section.5.11}{}}
\@writefile{toc}{\contentsline {chapter}{CHAPTER \numberline {6}Robust and interpretable: A neural approach }{120}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:emnlp2017}{{6}{120}{Robust and interpretable: A neural approach}{chapter.6}{}}
\citation{Zeiler2014VisualizingAU,nips15_hermann,Li2016VisualizingAU}
\citation{Lei2016RationalizingNP}
\citation{Bordes2015LargescaleSQ,nips15_hermann,He2016CharacterLevelQA,dong2015question,Tan2016ImprovedRL}
\citation{Iyyer2015,chen2016thorough,Parikh2016ADA}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Chapter Outline}{121}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Related Work}{121}{section.6.2}}
\newlabel{sec-emnlp2017:relatedwork}{{6.2}{121}{Related Work}{section.6.2}{}}
\citation{sachan2016science}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces  Architecture of our question answering approach. Given a question, candidate answer, and a free-text knowledge base as inputs, we generate a pool of candidate justifications, from which we extract feature vectors. We use a neural network to score each and then retain \textit  {only} the highest-scoring justification from the pool of candidates (i.e., max-pooling), thus selecting the current best justification. This justification score serves as the score for the candidate answer itself. The red border indicates the components that are trained online. }}{122}{figure.6.1}}
\newlabel{fig:arch_overall}{{6.1}{122}{Architecture of our question answering approach. Given a question, candidate answer, and a free-text knowledge base as inputs, we generate a pool of candidate justifications, from which we extract feature vectors. We use a neural network to score each and then retain \textit {only} the highest-scoring justification from the pool of candidates (i.e., max-pooling), thus selecting the current best justification. This justification score serves as the score for the candidate answer itself. The red border indicates the components that are trained online}{figure.6.1}{}}
\citation{chen2014fast}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Approach}{123}{section.6.3}}
\newlabel{sec-emnlp2017:approach}{{6.3}{123}{Approach}{section.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces  Detailed architecture of the model's scoring component. The question, candidate answer, and justification are encoded (by summing their word embeddings) to create vector representations of each. These representations are combined in several ways to create a set of representation-based similarity features that are concatenated to additional explicit features capturing lexical overlap, discourse and IR information and fed into a feed-forward neural network. The output layer of the network is a single node that represents the score of the justification candidate.}}{124}{figure.6.2}}
\newlabel{fig:arch}{{6.2}{124}{Detailed architecture of the model's scoring component. The question, candidate answer, and justification are encoded (by summing their word embeddings) to create vector representations of each. These representations are combined in several ways to create a set of representation-based similarity features that are concatenated to additional explicit features capturing lexical overlap, discourse and IR information and fed into a feed-forward neural network. The output layer of the network is a single node that represents the score of the justification candidate}{figure.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Model and Features}{124}{section.6.4}}
\newlabel{sec-emnlp2017:pipeline}{{6.4}{124}{Model and Features}{section.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces { Summary of the features calculated for each candidate justification. }}}{125}{table.6.1}}
\newlabel{tab:feature_examples}{{6.1}{125}{ Summary of the features calculated for each candidate justification}{table.6.1}{}}
\citation{Iyyer2015}
\citation{Bordes2013TranslatingEF}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Candidate Justification Retrieval}{126}{subsection.6.4.1}}
\newlabel{sec-emnlp2017:justretrieval}{{6.4.1}{126}{Candidate Justification Retrieval}{subsection.6.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Feature Extraction}{126}{subsection.6.4.2}}
\newlabel{sec-emnlp2017:features}{{6.4.2}{126}{Feature Extraction}{subsection.6.4.2}{}}
\citation{jansen14,sharp-EtAl:2015:NAACL-HLT,sachan2016science}
\citation{Surdeanu:15}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Example showing the discourse feature extracted from a question, answer, and justification. Words occurring in the question are shown in blue and words occurring in the answer are shown in green. The boxes around the justification text indicate the elementary discourse units identified by the discourse parser, and the arrow indicates the found discourse relation (with the assigned relation label given in red). }}{128}{figure.6.3}}
\newlabel{fig:discourseexample}{{6.3}{128}{Example showing the discourse feature extracted from a question, answer, and justification. Words occurring in the question are shown in blue and words occurring in the answer are shown in green. The boxes around the justification text indicate the elementary discourse units identified by the discourse parser, and the arrow indicates the found discourse relation (with the assigned relation label given in red)}{figure.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Neural Network}{129}{subsection.6.4.3}}
\newlabel{sec-emnlp2017:nn_model}{{6.4.3}{129}{Neural Network}{subsection.6.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Experiments}{130}{section.6.5}}
\newlabel{sec-emnlp2017:experiments}{{6.5}{130}{Experiments}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Data and Setup}{130}{subsection.6.5.1}}
\citation{manning2014stanford}
\citation{chen2014fast}
\citation{Surdeanu:15}
\citation{mikolov10,mikolov13}
\citation{levy2014dependency}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Baselines}{131}{subsection.6.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Corpora}{131}{subsection.6.5.3}}
\citation{chollet2015keras}
\citation{2016arXiv160502688short}
\citation{rmsprop}
\citation{Iyyer2015}
\citation{khot2017tupleinf}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces { Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1). $^*$s indicate that the difference between the corresponding model and the IR baseline is statistically significant ($^*$ indicates $p < 0.05$ and $^{**}$ indicates $p < 0.001$) and $^{\dagger }$s indicate significance compared to IR$^{++}$, All significance values were determined through a one-tailed bootstrap resampling test with 100,000 iterations. }}}{132}{table.6.2}}
\newlabel{tab:p@1}{{6.2}{132}{ Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1). $^*$s indicate that the difference between the corresponding model and the IR baseline is statistically significant ($^*$ indicates $p < 0.05$ and $^{**}$ indicates $p < 0.001$) and $^{\dagger }$s indicate significance compared to IR$^{++}$, All significance values were determined through a one-tailed bootstrap resampling test with 100,000 iterations}{table.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.4}Model Tuning}{132}{subsection.6.5.4}}
\citation{manning08}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces { Ablation of feature groups results, measured by precision-at-one (P@1) on validation data. Emb indicates our embedding-based features, LO indicates our lexical overlap features, lexDisc signifies our semi-lexicalized discourse features, and IR$^{++}$ indicates our information retrieval based features. Significance is indicated as in Table \ref  {tab:p@1}.}}}{133}{table.6.3}}
\newlabel{tab:ablation}{{6.3}{133}{ Ablation of feature groups results, measured by precision-at-one (P@1) on validation data. Emb indicates our embedding-based features, LO indicates our lexical overlap features, lexDisc signifies our semi-lexicalized discourse features, and IR$^{++}$ indicates our information retrieval based features. Significance is indicated as in Table \ref {tab:p@1}}{table.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Results}{133}{section.6.6}}
\newlabel{sec-emnlp2017:results}{{6.6}{133}{Results}{section.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}QA Performance}{133}{subsection.6.6.1}}
\newlabel{sec-emnlp2017:accuracy}{{6.6.1}{133}{QA Performance}{subsection.6.6.1}{}}
\citation{khot2017tupleinf}
\citation{sachan2016science}
\citation{Iyyer2015}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Previous Work:}{134}{section*.20}}
\citation{jansen2017framing}
\@writefile{toc}{\contentsline {paragraph}{Feature Contribution:}{135}{section*.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Justification Performance}{135}{subsection.6.6.2}}
\newlabel{sec-emnlp2017:justification_results}{{6.6.2}{135}{Justification Performance}{subsection.6.6.2}{}}
\citation{manning08}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces { Example justifications from the our model and their associated ratings. }}}{136}{table.6.4}}
\newlabel{tab:justification_examples}{{6.4}{136}{ Example justifications from the our model and their associated ratings}{table.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces {Percentage of questions that have at least one \emph  {good} justification within the top 1 (Good@1) and the top 5 (Good@5) justifications, as well as the normalized discounted cumulative gain at 5 (NDCG@5) of the ranked justifications. Significance indicated as in Table \ref  {tab:p@1}. }}}{137}{table.6.5}}
\newlabel{tab:justification_ndcg}{{6.5}{137}{Percentage of questions that have at least one \emph {good} justification within the top 1 (Good@1) and the top 5 (Good@5) justifications, as well as the normalized discounted cumulative gain at 5 (NDCG@5) of the ranked justifications. Significance indicated as in Table \ref {tab:p@1}}{table.6.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Contribution of Learning to Rerank Justifications:}{137}{section*.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Number of questions for which our complete model chooses a new justification at each epoch during training. While this is for a single random seed, we see essentially identical graphs for each random initialization.}}{138}{figure.6.4}}
\newlabel{fig:changes}{{6.4}{138}{Number of questions for which our complete model chooses a new justification at each epoch during training. While this is for a single random seed, we see essentially identical graphs for each random initialization}{figure.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.3}Error Analysis}{138}{subsection.6.6.3}}
\newlabel{sec-emnlp2017:erroranalysis}{{6.6.3}{138}{Error Analysis}{subsection.6.6.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces { Summary of the findings of the 30 question error analysis. Examples of several categories are provided in separate tables. Note that a given question may fall into more than one category.}}}{139}{table.6.6}}
\newlabel{tab:erroranalysis}{{6.6}{139}{ Summary of the findings of the 30 question error analysis. Examples of several categories are provided in separate tables. Note that a given question may fall into more than one category}{table.6.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces { Example of the system preferring a justification for which all the terms were found in either the question or answer candidate, while the justification for the correct answer contained additional information necessary to fully explain the answer. (Justifications shown in italics) }}}{139}{table.6.7}}
\newlabel{tab:ex_lex_overlap}{{6.7}{139}{ Example of the system preferring a justification for which all the terms were found in either the question or answer candidate, while the justification for the correct answer contained additional information necessary to fully explain the answer. (Justifications shown in italics)}{table.6.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.8}{\ignorespaces { Example of a question for which complex inference is required. In order to answer the question, you would need to assemble the event chain: cut grass left on the ground $\rightarrow $ grass decomposes $\rightarrow $ decomposed material provides nutrients.}}}{140}{table.6.8}}
\newlabel{tab:ex_complex_inf}{{6.8}{140}{ Example of a question for which complex inference is required. In order to answer the question, you would need to assemble the event chain: cut grass left on the ground $\rightarrow $ grass decomposes $\rightarrow $ decomposed material provides nutrients}{table.6.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.9}{\ignorespaces { Example of a question for which knowledge base noise (here, in the form of over-generalization) was an issue.}}}{140}{table.6.9}}
\newlabel{tab:ex_noise}{{6.9}{140}{ Example of a question for which knowledge base noise (here, in the form of over-generalization) was an issue}{table.6.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Conclusion}{141}{section.6.7}}
\newlabel{sec-emnlp2017:conclusions}{{6.7}{141}{Conclusion}{section.6.7}{}}
\@input{mainmatter/conclusion.aux}
\@input{mainmatter/appendix_rules.aux}
\bibstyle{uabibnat}
\bibdata{mainmatter/refs}
\bibcite{arXivstats}{{1}{2017}{{arXiv}}{{}}}
\bibcite{balduccini2008knowledge}{{2}{2008}{{Balduccini et~al.}}{{Balduccini, Baral, and Lierler}}}
\bibcite{banarescu2012amr}{{3}{2013}{{Banarescu et~al.}}{{Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, and Schneider}}}
\bibcite{baral2012knowledge}{{4}{2012}{{Baral and Liang}}{{}}}
\bibcite{baral2011towards}{{5}{2011}{{Baral et~al.}}{{Baral, Liang, and Nguyen}}}
\bibcite{baral2012answering}{{6}{2012}{{Baral et~al.}}{{Baral, Vo, and Liang}}}
\bibcite{barzilay2005sentence}{{7}{2005}{{Barzilay and McKeown}}{{}}}
\bibcite{barzilay1999information}{{8}{1999}{{Barzilay et~al.}}{{Barzilay, McKeown, and Elhadad}}}
\bibcite{Berger:00}{{9}{2000}{{Berger et~al.}}{{Berger, Caruana, Cohn, Freytag, and Mittal}}}
\bibcite{bjorkelund2014learning}{{10}{2014}{{Bj{\"o}rkelund and Kuhn}}{{}}}
\bibcite{blair2003hybrid}{{11}{2003}{{Blair-Goldensohn et~al.}}{{Blair-Goldensohn, McKeown, and Schlaikjer}}}
\bibcite{bordes2014question}{{12}{2014}{{Bordes et~al.}}{{Bordes, Chopra, and Weston}}}
\@writefile{toc}{\contentsline {chapter}{REFERENCES}{153}{chapter*.23}}
\bibcite{Bordes2015LargescaleSQ}{{13}{2015}{{Bordes et~al.}}{{Bordes, Usunier, Chopra, and Weston}}}
\bibcite{Bordes2013TranslatingEF}{{14}{2013}{{Bordes et~al.}}{{Bordes, Usunier, Garc{\'i}a-Dur{\'a}n, Weston, and Yakhnenko}}}
\bibcite{Brown:93}{{15}{1993}{{Brown et~al.}}{{Brown, Pietra, Pietra, and Mercer}}}
\bibcite{brysbaert:2014}{{16}{2014}{{Brysbaert et~al.}}{{Brysbaert, Warriner, and Kuperman}}}
\bibcite{chambers2007learning}{{17}{2007}{{Chambers et~al.}}{{Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, De~Marneffe, Ramage, Yeh, and Manning}}}
\bibcite{chen2016thorough}{{18}{2016}{{Chen et~al.}}{{Chen, Bolton, and Manning}}}
\bibcite{chen14}{{19}{2014{a}}{{Chen and Manning}}{{}}}
\bibcite{chen2014fast}{{20}{2014{b}}{{Chen and Manning}}{{}}}
\bibcite{chollet2015keras}{{21}{2015}{{Chollet}}{{}}}
\bibcite{chu2004ibm}{{22}{2004}{{Chu-Carroll et~al.}}{{Chu-Carroll, Czuba, Prager, Ittycheriah, and Blair-Goldensohn}}}
\bibcite{clark:2015}{{23}{2015}{{Clark}}{{}}}
\bibcite{clark:2013}{{24}{2013{a}}{{Clark et~al.}}{{Clark, Harrison, and Balasubramanian}}}
\bibcite{clark2013study}{{25}{2013{b}}{{Clark et~al.}}{{Clark, Harrison, and Balasubramanian}}}
\bibcite{cole2005lightweight}{{26}{2005}{{Cole et~al.}}{{Cole, Royal, Valtorta, Huhns, and Bowles}}}
\bibcite{Collins:2002:DTM}{{27}{2002}{{Collins}}{{}}}
\bibcite{craven1996extracting}{{28}{1996}{{Craven and Shavlik}}{{}}}
\bibcite{Cunningham2011a}{{29}{2011}{{Cunningham et~al.}}{{Cunningham, Maynard, Bontcheva, Tablan, Aswani, Roberts, Gorrell, Funk, Roberts, Damljanovic, Heitz, Greenwood, Saggion, Petrak, Li, and Peters}}}
\bibcite{curran2007minimising}{{30}{2007}{{Curran et~al.}}{{Curran, Murphy, and Scholz}}}
\bibcite{daume2007}{{31}{2007}{{Daum{\'e}~III}}{{}}}
\bibcite{de2008stanford}{{32}{2008}{{De~Marneffe and Manning}}{{}}}
\bibcite{do2011minimally}{{33}{2011}{{Do et~al.}}{{Do, Chan, and Roth}}}
\bibcite{dong2015question}{{34}{2015}{{Dong et~al.}}{{Dong, Wei, Zhou, and Xu}}}
\bibcite{du2017question}{{35}{2017}{{Du et~al.}}{{Du, Shao, and Cardie}}}
\bibcite{echihabi2003noisy}{{36}{2003{a}}{{Echihabi and Marcu}}{{}}}
\bibcite{Echihabi:03}{{37}{2003{b}}{{Echihabi and Marcu}}{{}}}
\bibcite{Etzioni:11}{{38}{2011}{{Etzioni}}{{}}}
\bibcite{faruqui2016problems}{{39}{2016}{{Faruqui et~al.}}{{Faruqui, Tsvetkov, Rastogi, and Dyer}}}
\bibcite{feng12}{{40}{2012}{{Feng and Hirst}}{{}}}
\bibcite{fernandes2012latent}{{41}{2012}{{Fernandes et~al.}}{{Fernandes, Dos~Santos, and Milidi{\'u}}}}
\bibcite{ferrucci2010building}{{42}{2010}{{Ferrucci et~al.}}{{Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager et~al.}}}
\bibcite{finkel2010hierarchical}{{43}{2010}{{Finkel and Manning}}{{}}}
\bibcite{fitzgerald2015semantic}{{44}{2015}{{FitzGerald et~al.}}{{FitzGerald, T{\"a}ckstr{\"o}m, Ganchev, and Das}}}
\bibcite{fried2015higher}{{45}{2015}{{Fried et~al.}}{{Fried, Jansen, Hahn-Powell, Surdeanu, and Clark}}}
\bibcite{girju2002text}{{46}{2002}{{Girju and Moldovan}}{{}}}
\bibcite{halliday2014cohesion}{{47}{2014}{{Halliday and Hasan}}{{}}}
\bibcite{Harabagiu:00}{{48}{2000}{{Harabagiu et~al.}}{{Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, and Morarescu}}}
\bibcite{He2016CharacterLevelQA}{{49}{2016}{{He and Golub}}{{}}}
\bibcite{hearst1992automatic}{{50}{1992}{{Hearst}}{{}}}
\bibcite{Heilman2010GQS}{{51}{2010}{{Heilman and Smith}}{{}}}
\bibcite{hendrickx2009semeval}{{52}{2009}{{Hendrickx et~al.}}{{Hendrickx, Kim, Kozareva, Nakov, {\'O}~S{\'e}aghdha, Pad{\'o}, Pennacchiotti, Romano, and Szpakowicz}}}
\bibcite{nips15_hermann}{{53}{2015}{{Hermann et~al.}}{{Hermann, Ko\v {c}isk\'y, Grefenstette, Espeholt, Kay, Suleyman, and Blunsom}}}
\bibcite{hernault10}{{54}{2010}{{Hernault et~al.}}{{Hernault, Prendinger, duVerle, and Ishizuka}}}
\bibcite{hickl2006recognizing}{{55}{2006}{{Hickl et~al.}}{{Hickl, Williams, Bensley, Roberts, Rink, and Shi}}}
\bibcite{hochreiter1997long}{{56}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hoffmann2011knowledge}{{57}{2011}{{Hoffmann et~al.}}{{Hoffmann, Zhang, Ling, Zettlemoyer, and Weld}}}
\bibcite{iyyer2015deep}{{58}{2015{a}}{{Iyyer et~al.}}{{Iyyer, Manjunatha, Boyd-Graber, and Daum{\'e}~III}}}
\bibcite{Iyyer2015}{{59}{2015{b}}{{Iyyer et~al.}}{{Iyyer, Manjunatha, Boyd-Graber, and {Daum\'{e} III}}}}
\bibcite{jansen-EtAl:2016:COLING}{{60}{2016}{{Jansen et~al.}}{{Jansen, Balasubramanian, Surdeanu, and Clark}}}
\bibcite{jansen2017framing}{{61}{2017}{{Jansen et~al.}}{{Jansen, Sharp, Surdeanu, and Clark}}}
\bibcite{jansen14}{{62}{2014}{{Jansen et~al.}}{{Jansen, Surdeanu, and Clark}}}
\bibcite{Khashabi2016QuestionAV}{{63}{2016}{{Khashabi et~al.}}{{Khashabi, Khot, Sabharwal, Clark, Etzioni, and Roth}}}
\bibcite{khoo1998automatic}{{64}{1998}{{Khoo et~al.}}{{Khoo, Kornfilt, Oddy, and Myaeng}}}
\bibcite{khot2017tupleinf}{{65}{2017}{{Khot et~al.}}{{Khot, Sabharwal, and Clark}}}
\bibcite{kielaspecializing}{{66}{2015}{{Kiela et~al.}}{{Kiela, Hill, and Clark}}}
\bibcite{Kim2015MindTG}{{67}{2015}{{Kim et~al.}}{{Kim, Shah, and Doshi-Velez}}}
\bibcite{kingma2014adam}{{68}{2014}{{Kingma and Ba}}{{}}}
\bibcite{Le2014DistributedRO}{{69}{2014}{{Le and Mikolov}}{{}}}
\bibcite{Lei2016RationalizingNP}{{70}{2016}{{Lei et~al.}}{{Lei, Barzilay, and Jaakkola}}}
\bibcite{letham2015interpretable}{{71}{2015}{{Letham et~al.}}{{Letham, Rudin, McCormick, Madigan et~al.}}}
\bibcite{levy2014dependency}{{72}{2014}{{Levy and Goldberg}}{{}}}
\bibcite{levy2015supervised}{{73}{2015}{{Levy et~al.}}{{Levy, Remus, Biemann, Dagan, and Ramat-Gan}}}
\bibcite{lewis2013combining}{{74}{2013}{{Lewis and Steedman}}{{}}}
\bibcite{Li2016VisualizingAU}{{75}{2016}{{Li et~al.}}{{Li, Chen, Hovy, and Jurafsky}}}
\bibcite{liang2006end}{{76}{2006}{{Liang et~al.}}{{Liang, Bouchard-C{\^o}t{\'e}, Klein, and Taskar}}}
\bibcite{liang2013learning}{{77}{2013}{{Liang et~al.}}{{Liang, Jordan, and Klein}}}
\bibcite{maccartney2009natural}{{78}{2009}{{MacCartney}}{{}}}
\bibcite{mann88}{{79}{1988}{{Mann and Thompson}}{{}}}
\bibcite{manning08}{{80}{2008}{{Manning et~al.}}{{Manning, Raghavan, and Sch\"{u}tze}}}
\bibcite{Manning:14}{{81}{2014{a}}{{Manning et~al.}}{{Manning, Surdeanu, Bauer, Finkel, Bethard, and McClosky}}}
\bibcite{manning2014stanford}{{82}{2014{b}}{{Manning et~al.}}{{Manning, Surdeanu, Bauer, Finkel, Bethard, and McClosky}}}
\bibcite{marcu97}{{83}{1997}{{Marcu}}{{}}}
\bibcite{mcsherry2008}{{84}{2008}{{McSherry and Najork}}{{}}}
\bibcite{mikolov13}{{85}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{mikolov10}{{86}{2010}{{Mikolov et~al.}}{{Mikolov, Karafiat, Burget, Cernocky, and Khudanpur}}}
\bibcite{mikolov2013distributed}{{87}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{moldovan2007cogex}{{88}{2007}{{Moldovan et~al.}}{{Moldovan, Clark, Harabagiu, and Hodges}}}
\bibcite{moldovan2003cogex}{{89}{2003{a}}{{Moldovan et~al.}}{{Moldovan, Clark, Harabagiu, and Maiorano}}}
\bibcite{Moldovan:2003:PIE:763693.763694}{{90}{2003{b}}{{Moldovan et~al.}}{{Moldovan, Pa\c {s}ca, Harabagiu, and Surdeanu}}}
\bibcite{moldovan2001logic}{{91}{2001}{{Moldovan and Rus}}{{}}}
\bibcite{Napoles2012}{{92}{2012{a}}{{Napoles et~al.}}{{Napoles, Gormley, and Van~Durme}}}
\bibcite{napoles2012annotated}{{93}{2012{b}}{{Napoles et~al.}}{{Napoles, Gormley, and Van~Durme}}}
\bibcite{och03}{{94}{2003}{{Och and Ney}}{{}}}
\bibcite{oh2013question}{{95}{2013}{{Oh et~al.}}{{Oh, Torisawa, Hashimoto, Sano, De~Saeger, and Ohtake}}}
\bibcite{Pagliardini2017UnsupervisedLO}{{96}{2017}{{Pagliardini et~al.}}{{Pagliardini, Gupta, and Jaggi}}}
\bibcite{Parikh2016ADA}{{97}{2016}{{Parikh et~al.}}{{Parikh, T{\"a}ckstr{\"o}m, Das, and Uszkoreit}}}
\bibcite{Park:2015}{{98}{2015}{{Park and Croft}}{{}}}
\bibcite{Piaget1954}{{99}{1954}{{Piaget}}{{}}}
\bibcite{pradhan2002building}{{100}{2002}{{Pradhan et~al.}}{{Pradhan, Krugler, Bethard, Ward, Jurafsky, Martin, Blair-Goldensohn, Schlaikjer, Filatova, Dubou{\'e} et~al.}}}
\bibcite{internetstats}{{101}{2017}{{Real Time Statistics Project}}{{}}}
\bibcite{Reece:2011}{{102}{2011}{{Reece et~al.}}{{Reece, Urry, Cain, Wasserman, and Minorsky}}}
\bibcite{Ribeiro2016WhySI}{{103}{2016}{{Ribeiro et~al.}}{{Ribeiro, Singh, and Guestrin}}}
\bibcite{riedel2013relation}{{104}{2013}{{Riedel et~al.}}{{Riedel, Yao, McCallum, and Marlin}}}
\bibcite{Riezler:etal:2007}{{105}{2007}{{Riezler et~al.}}{{Riezler, Vasserman, Tsochantaridis, Mittal, and Liu}}}
\bibcite{riloff1996automatically}{{106}{1996}{{Riloff}}{{}}}
\bibcite{sachan2016science}{{107}{2016}{{Sachan et~al.}}{{Sachan, Dubey, and Xing}}}
\bibcite{Serban2016GeneratingFQ}{{108}{2016}{{Serban et~al.}}{{Serban, Garc{\'{\i }}a-Dur{\'{a}}n, G{\"{u}}l{\c c}ehre, Ahn, Chandar, Courville, and Bengio}}}
\bibcite{Severyn:12}{{109}{2012}{{Severyn and Moschitti}}{{}}}
\bibcite{Severyn:13a}{{110}{2013}{{Severyn and Moschitti}}{{}}}
\bibcite{Severyn2015LearningTR}{{111}{2015}{{Severyn and Moschitti}}{{}}}
\bibcite{Severyn:13b}{{112}{2013}{{Severyn et~al.}}{{Severyn, Nicosia, and Moschitti}}}
\bibcite{sharmatowards}{{113}{2015}{{Sharma et~al.}}{{Sharma, Vo, Aditya, and Baral}}}
\bibcite{sharp-EtAl:2015:NAACL-HLT}{{114}{2015}{{Sharp et~al.}}{{Sharp, Jansen, Surdeanu, and Clark}}}
\bibcite{Shen:Joshi:2005}{{115}{2005}{{Shen and Joshi}}{{}}}
\bibcite{Soricut:06}{{116}{2006}{{Soricut and Brill}}{{}}}
\bibcite{sultan-etal:2014:TACL}{{117}{2014}{{Sultan et~al.}}{{Sultan, Bethard, and Sumner}}}
\bibcite{sun2009latent}{{118}{2009}{{Sun et~al.}}{{Sun, Matsuzaki, Okanohara, and Tsujii}}}
\bibcite{Surdeanu:11}{{119}{2011}{{Surdeanu et~al.}}{{Surdeanu, Ciaramita, and Zaragoza}}}
\bibcite{Surdeanu:15}{{120}{2015}{{Surdeanu et~al.}}{{Surdeanu, Hicks, and Valenzuela-Esc\'{a}rcega}}}
\bibcite{Sutskever2014SequenceTS}{{121}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{Tan2016ImprovedRL}{{122}{2016}{{Tan et~al.}}{{Tan, dos Santos, Xiang, and Zhou}}}
\bibcite{baral2006using}{{123}{2006}{{Tari and Baral}}{{}}}
\bibcite{2016arXiv160502688short}{{124}{2016}{{Theano Development Team}}{{}}}
\bibcite{rmsprop}{{125}{2012}{{Tieleman and Hinton}}{{}}}
\bibcite{valenzuela2016runes}{{126}{2016}{{Valenzuela-Esc\'{a}rcega et~al.}}{{Valenzuela-Esc\'{a}rcega, Hahn-Powell, and Surdeanu}}}
\bibcite{Verberne:2007}{{127}{2007}{{Verberne et~al.}}{{Verberne, Boves, Oostdijk, Coppen et~al.}}}
\bibcite{Voorhees:2003}{{128}{2003}{{Voorhees}}{{}}}
\bibcite{wang2015long}{{129}{2015}{{Wang and Nyberg}}{{}}}
\bibcite{Wang2010ProbabilisticTM}{{130}{2010}{{Wang and Manning}}{{}}}
\bibcite{wiki:wikistats}{{131}{2017}{{Wikipedia}}{{}}}
\bibcite{woodsenddistributed}{{132}{2015}{{Woodsend and Lapata}}{{}}}
\bibcite{yang2014joint}{{133}{2014}{{Yang et~al.}}{{Yang, Duan, Zhou, and Rim}}}
\bibcite{yang2014multi}{{134}{2014}{{Yang and Mao}}{{}}}
\bibcite{yao2013}{{135}{2013}{{Yao et~al.}}{{Yao, Van~Durme, Callison-Burch, and Clark}}}
\bibcite{yih13}{{136}{2013}{{Yih et~al.}}{{Yih, Chang, Meek, and Pastusiak}}}
\bibcite{Zeiler2014VisualizingAU}{{137}{2014}{{Zeiler and Fergus}}{{}}}
\bibcite{zettlemoyer2007online}{{138}{2007}{{Zettlemoyer and Collins}}{{}}}
