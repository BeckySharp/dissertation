\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces In this work we focus on two desirable aspects of a question answering approach: robustness and interpretability. Here we plot several approaches, including the four that are the focus of this work (the colored dots, please see corresponding chapters for model details), in terms of these aspects. Note that the ideal model would be found in the upper right-hand corner -- robust to language variation and easily interpretable by a human user.}}{18}{figure.1.1}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces {\relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip An example of the alignments produced by the two discourse models. The sequential model aligns pairs of consecutive sentences, capturing intersentence associations such as \emph {cider--apples}, and \emph {orchard--autumn}. The Rhetorical Structure Theory based model generates alignment pairs from participants in all (binary) discourse relations, capturing both intrasentence and intersentence alignments, including \emph {apples--orchard, cider--apples}, and \emph {cider--autumn}.}}}{40}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces {\relax \fontsize {10.95}{13.6}\selectfont \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Overall performance for the two discourse-based alignment models (the Rhetorical Structure Theory based model and the sequential model), compared against the IR baseline, random baseline, and an embedding-based reranker. The $x$ axis indicates the number of training documents used to construct all models. Each point represents the average of 10 samples of training documents. }}}{46}{figure.3.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces { Architecture of the causal convolutional network. }}}{59}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces {Precision-recall curve showing the ability of each model to rank causal pairs above non-causal pairs. The Look-up model has no data points beyond the 35\% recall point.}}}{63}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces {Illustration of the indirect learning of associations that facilitates approximate inference. Note that in this illustrated example, the learned associations are noisy, but this mechanism is critical -- it is what allows embedding models to generalize beyond the explicit training examples. The blue highlight indicates words embedded in the cause (target) space, and the yellow indicates words embedded in the effect (context) space.}}}{64}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces {Precision-recall curve including the bidirectional variants of the causal models. For clarity, we do not plot the basic noise aware causal embedding model (cEmbedNoise), which performs worse than its bidirectional variant (cEmbedBiNoise, described in Section \ref {sec-emnlp2016:results}). }}}{66}{figure.4.4}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Our QA approach, which centers on the construction of answer justifications as text aggregation graphs, and ranking them using a model that treats the justification quality as a latent variable.}}{77}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Five example graphlets for five sentences that could be aggregated together in different combinations to justifiably answer the question \emph {What tools could determine the speed of turtles walking along a path? (Answer: stopwatch and meter stick)}. Each graphlet contains one or more information nuggets (grey boxes) composed of one or more terms. For example, the graphlet for the sentence \emph {A stopwatch can be used to measure time} contains two information nuggets. Edges between nuggets within a graphlet are shown with arrows, where a subset of these edges are labelled (e.g., \emph {EXAMPLE, INSTRUMENT}), and the rest are unlabelled. }}{83}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Two example Text Aggregation Graphs (TAGs) that justifiably answer the question \emph {What tools could determine the speed of turtles walking along a path} for the answer \emph {a stopwatch and meter stick}. Asterisks (*) denote that a given term is either a question or answer focus word, while pounds (\#) denote terms that are not found in the question or answer, but which are shared between graphlets. Links between the graphlets in a given TAG are highlighted. (top) A two-sentence TAG, where the edges between graphlets connect on a focus word \emph {(speed)} and other words shared between the graphlets \emph {(distance, measure)}. (bottom) A three-sentence TAG, where the edges between graphlets connect entirely on shared words between the graphlets \emph {(distance, measure, time)} that are not focus words. }}{85}{figure.5.3}
\contentsline {figure}{\numberline {5.4}{\ignorespaces {Connections between sentences are characterized based on lexical overlap between graphlets. Here, each box represents a two-sentence TAG, with graphlets stacked vertically. The presence of question or answer focus words is marked with \emph {Q} or \emph {A}, while the presence of other non-focus words shared between the two graphlets is marked with \emph {X}. Lexical overlap within a category is highlighted. }}}{88}{figure.5.4}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Architecture of our question answering approach. Given a question, candidate answer, and a free-text knowledge base as inputs, we generate a pool of candidate justifications, from which we extract feature vectors. We use a neural network to score each and then retain \textit {only} the highest-scoring justification from the pool of candidates (i.e., max-pooling), thus selecting the current best justification. This justification score serves as the score for the candidate answer itself. The red border indicates the components that are trained online. }}{122}{figure.6.1}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Detailed architecture of the model's scoring component. The question, candidate answer, and justification are encoded (by summing their word embeddings) to create vector representations of each. These representations are combined in several ways to create a set of representation-based similarity features that are concatenated to additional explicit features capturing lexical overlap, discourse and IR information and fed into a feed-forward neural network. The output layer of the network is a single node that represents the score of the justification candidate.}}{124}{figure.6.2}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Example showing the discourse feature extracted from a question, answer, and justification. Words occurring in the question are shown in blue and words occurring in the answer are shown in green. The boxes around the justification text indicate the elementary discourse units identified by the discourse parser, and the arrow indicates the found discourse relation (with the assigned relation label given in red). }}{128}{figure.6.3}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Number of questions for which our complete model chooses a new justification at each epoch during training. While this is for a single random seed, we see essentially identical graphs for each random initialization.}}{138}{figure.6.4}
\addvspace {10\p@ }
\addvspace {10\p@ }
